[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lucas Okwudishu",
    "section": "",
    "text": "Bio\n\nHi I’m Lucas!\nI am a data and analytics professional with over 10 years of experience in business operations and analytics. I have expertise in multiple industries including retail, manufacturing, non-profit and marketing.\nI have a proven track record of improving business operations and driving ROI true insights and action.\nI love digging into data to understand how various business components affect top-line revenue. This helps with better revenue modeling as well as understanding what levers can be used to drive revenue while eliminating inefficiencies.\n\n\n\n\nExperience & Education\n\nAs a Senior Data Analyst at HubSpot, I leverage data to refine and bolster our marketing strategies.\nMy professional journey has also included analytic roles at United Way Worldwide, DAP, and Whole Foods Market. The consistent application of data intelligence has been the cornerstone of my professional journey.\nI also hold a masters degree in Applied Economics from Johns Hopkins University, and a certificate in Business Analytics from George Washington University.\nPlease contact me for a copy of my full resume.\n\n\n\n\nOpen Source Contribution\nI am currently contributing to the development of Pytimetk, an open source python package for time series analysis."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html",
    "href": "projects/customer_segmentation_banking/index.html",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "",
    "text": "The banking industry struggles with understanding and meeting the diverse needs of customers, resulting in generic marketing approaches to financial product offerings that fall short of creating personalized experiences. To address this challenge, a data-driven solution is required to effectively segment customers and create targeted personas for improved customer engagement and satisfaction, thus leading to increased revenue for banks/financial institutions.\n\n\nCustomer segmentation can be achieved through the powerful k-means algorithm. By leveraging data analytics and machine learning techniques, financial institutions (banks & credit card companies) gain valuable insights into customer behavior, preferences, and characteristics. This enables the identification of distinct customer segments and the development of personalized offerings. This has several advantages including enhanced customer understanding, improved customer engagement with product offerings, a healthier customer base, and increased revenue for financial institutions.\n\n\n\nTo implement customer segmentation and using the k-means algorithm, financial institutions should follow these steps;\n\nData Collection and Integration: Gather customer data from various sources, including transaction records, demographic information, and customer interactions.\nPreprocessing and Feature Selection: Cleanse and preprocess the data, selecting relevant features for analysis, such as transaction frequency, average balance, age, and customer preferences.\nK-Means Clustering: Apply the k-means algorithm to segment customers based on similar behavioral patterns. Determine the optimal number of clusters and assign customers to their respective segments.\nPersona Creation: Develop personas for each customer segment, incorporating demographic information, behaviors, preferences, and goals. These personas should be representative of the segment’s characteristics and serve as a guide for targeted marketing strategies.\nImplementation and Iteration: Implement personalized marketing campaigns and track the performance of the tailored strategies. Continuously evaluate and refine the segments and personas based on customer feedback and evolving market dynamics.\n\nThe following pages, walk through a detailed case study of customer segmentation and persona creation using the k-means algorithm.\n\n\n\n\n\n\nNote\n\n\n\nShorter excerpt available on Medium"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#solution",
    "href": "projects/customer_segmentation_banking/index.html#solution",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "",
    "text": "Customer segmentation can be achieved through the powerful k-means algorithm. By leveraging data analytics and machine learning techniques, financial institutions (banks & credit card companies) gain valuable insights into customer behavior, preferences, and characteristics. This enables the identification of distinct customer segments and the development of personalized offerings. This has several advantages including enhanced customer understanding, improved customer engagement with product offerings, a healthier customer base, and increased revenue for financial institutions."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#implementation-strategy",
    "href": "projects/customer_segmentation_banking/index.html#implementation-strategy",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "",
    "text": "To implement customer segmentation and using the k-means algorithm, financial institutions should follow these steps;\n\nData Collection and Integration: Gather customer data from various sources, including transaction records, demographic information, and customer interactions.\nPreprocessing and Feature Selection: Cleanse and preprocess the data, selecting relevant features for analysis, such as transaction frequency, average balance, age, and customer preferences.\nK-Means Clustering: Apply the k-means algorithm to segment customers based on similar behavioral patterns. Determine the optimal number of clusters and assign customers to their respective segments.\nPersona Creation: Develop personas for each customer segment, incorporating demographic information, behaviors, preferences, and goals. These personas should be representative of the segment’s characteristics and serve as a guide for targeted marketing strategies.\nImplementation and Iteration: Implement personalized marketing campaigns and track the performance of the tailored strategies. Continuously evaluate and refine the segments and personas based on customer feedback and evolving market dynamics.\n\nThe following pages, walk through a detailed case study of customer segmentation and persona creation using the k-means algorithm.\n\n\n\n\n\n\nNote\n\n\n\nShorter excerpt available on Medium"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#k-means-algorithm",
    "href": "projects/customer_segmentation_banking/index.html#k-means-algorithm",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.1 K-Means Algorithm",
    "text": "5.1 K-Means Algorithm\nK-Means algorithm is a popular unsupervised machine learning technique used for clustering data. The algorithm works by dividing a set of data points into k clusters based on their similarity. Initially, k centroids are chosen randomly from the data points, and each data point is assigned to the nearest centroid. Then, the centroids are moved to the center of their respective clusters, and the data points are re-assigned to the nearest centroid. This process is repeated until the centroids no longer move or a maximum number of iterations is reached.\n\n\n\n\n\nThe k-means algorithm is widely used in various applications, such as customer segmentation, and anomaly detection. To learn more about the k-means algorithm, you can read up on the following sources:\n\nK-Means Clustering.\nBenefits & challenges of k-means for customer segmentation\nHow to use k-means clustering for customer segmentation\n\nBefore proceeding with k-means clustering, I decided to filter the dataset down to a particular cohort of customers, in order to ease computational time and ease of analyzing/visualizing clusters. I’ll be picking customers with tenure at 10 months for my analysis cohort. This cohort has 236 customers and makes up 2.6% of the dataset.\n\n5.1.1 Data Prep\nTo prepare the dataset for k-means clustering, I performed the following steps:\n\nCreate an analysis cohort by filtering for customers with tenure = 10.\nDrop any rows with null values. This leaves an analysis cohort of 226 customers.\nCreate a credit_utilization feature by dividing balance by credit_limit.\n\n\n\n5.1.2 Feature Engineering\nFeature engineering is an important step in preparing data for k-means clustering. I used the following techniques for feature engineering in order to improve the accuracy and effectiveness of the algorithm:\n\nRemove unwanted features - Remove the cust_id column. This is not needed for k-means and is used only as a customer identifier. I also make the executive decision to remove the installment_purchases column. This feature is defined as the “amount of purchases done in installments”. I would assume that purchases is the important feature in this dataset and regardless of the number of purchase installments.\nNormalization - This step scales each feature to have a mean of zero and standard deviation of one. This helps prevent features with larger values from dominating the clustering results.\nDimensionality Reduction (PCA) - Reducing the dimensionality of the data can reduce the computational complexity and improve the interpretability of the clustering results. A popular dimensionality reduction technique is called PCA (Principal Component Analysis). PCA works by identifying a smaller number of variables, known as principal components, that can explain the maximum amount of variance in the original dataset. These principal components are linear combinations of the original variables, and each one represents a different direction in the data. The first principal component captures the most variance in the data, while each subsequent component captures as much of the remaining variance as possible. For a video explanation of how PCA works, see this link.\n\nSetting a PCA threshold of 80%, meaning generating enough components to capture 80% of the variability in the features, PCA identified 5 principal components from 16 features. See sample below;\n\n\n\n\n\n\n\n5.1.3 Optimal Number of Clusters\nTo determine the optimal number of clusters, we can use a Skree Plot. A skree plot is used to determine the appropriate number of clusters to use in a k-means clustering analysis. The skree plot displays the within-cluster sum of squares (WCSS) on the y-axis, plotted against the number of clusters on the x-axis.\nThe within-cluster sum of squares measures the total squared distance between each point and it’s assigned cluster center, summed over all clusters. The scree plot helps identify the elbow point, or the point on the plot where adding more clusters does not significantly improve the clustering performance, i.e., the WCSS is not significantly reduced by adding more clusters.\nThe elbow point is typically used as a guide for deciding how many clusters to retain in the analysis. However, there is no hard and fast rule for selecting the number of clusters, this can be based on domain knowledge and knowledge about the dataset. The plot below is the skree plot for our analysis cohort;\n\n\n\n\n\nObservation: We can see that the elbow point appears to be 5 clusters. However after running the first iteration of the k-means algorithm, I noticed that only 1 customer (cust_id C11004) was being assigned to cluster 1. I would assume that this customer has certain characteristics that stand out for the rest of the cohort. For this particular analysis, I just decided to exclude this customer and re-cluster again, using 4 clusters."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#visualizing-clusters",
    "href": "projects/customer_segmentation_banking/index.html#visualizing-clusters",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.2 Visualizing Clusters",
    "text": "5.2 Visualizing Clusters\nUMAP (Uniform Manifold Approximation & Projection) is a machine learning technique for dimensionality reduction and data visualization. In order to visualize the cluster assignments, I used UMAP for the sole purpose of generating x and y axis coordinates for visualizing clusters. Recall our dataset for k-means was reduced to 5 principal components. I wanted to still be able to visualize the clusters in 2 dimensions.\nThe plot below shows our cluster assignments from the k-means algorithm;\n\n\n\n\n\nObservation: Although not perfect, the k-means algorithm does a decent job of segmenting customers. We can see 4 distinct segments with a few customers that should probably be re-clustered, although a 3 dimensional plot may present a different perspective."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#cluster-analysis",
    "href": "projects/customer_segmentation_banking/index.html#cluster-analysis",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.3 Cluster Analysis",
    "text": "5.3 Cluster Analysis\nNow that we have our clusters of customers, we can begin analyzing these clusters to understand trends in customer characteristics of the different clusters. Note that we have already seen some of these trends during the explanatory data analysis phase, thus we can better understand how the k-means algorithm is working. To start, we can plot the distribution of some features of interest by cluster using boxplots. Let’s start by plotting the distribution of balance, cash_advance, credit_utilization and purchases.\n\n\n\n\n\nObservation: Looking at the plot above, 2 distinct clusters stand out at first glance, cluster 4 and cluster 2:\n\nCluster 4: Include customers with high balance, cash_advance and credit_utilization. However, these customers have very low purchases. As we outlined earlier, these are customers that appear to be using their credit cards as a means to take out loans.\nCluster 2: Includes customers with high credit_utilization, slightly high balance and high purchases. These customers appear to be using their credits strictly for making purchases, and tend to pay off the balance frequently.\nCluster 3: These customers have lower values for all these features. However we know that this analysis cohort includes customers with tenure of 10 months, so these customers have had their credit cards for a while. This clusters appears to include customers who are cautious about using their credit cards and tend to make some low value purchases.\nCluster 1: These customers are similar to those in cluster 2, however they tend to have higher credit utilization. These customers appear to have much lower credit_limit (median of $2,500) compared to cluster 1 ($5,000) and cluster 3 ($3,500). Cluster 2 has the lowest median credit_limit ($1,350).\n\nLet us take a look at a few other feature distributions for our clusters. Below I show boxplots for credit_limit, minimum_payments, payments and purchase_trx for each cluster.\n\n\n\n\n\nObservation: Once again we can gleam a few insights:\n\nCluster 4: Appears to have higher credit_limit and also, customers in this cluster tend to make make higher minimum payments, suggesting that some of these customers (who are using their credit cards to take out loans) are indeed making efforts to pay off their balance.\nCluster 2: These customers have high the highest purchase_trx but also have payments,\nminimum_payment, credit_limit and similar to clusters 1 and 4. These customers while making frequent purchases with their credit tend to pay just the minimum required payments and thus always maintain some balance on their credit card.\nClusters 1 and 3: These customers appear to have similar characteristics and maintain low levels of use for their credit cards."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#customer-persona-product-recommendations",
    "href": "projects/customer_segmentation_banking/index.html#customer-persona-product-recommendations",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.4 Customer Persona / Product Recommendations",
    "text": "5.4 Customer Persona / Product Recommendations\nBased on the analysis in the previous section. We can thus create customer personas and come up with product recommendations to be marketed to these customers. We’ll do this using the following format; Cluster, Persona Name, Description, Stats, Product Recommendations.\n\nCluster 4: Loanee Customer\n\nDescription: These customers tend to use their credit cards a means for taking out loans. They use their credit card to bridge the gap between their financial needs, thus resulting in high cash advances, high balance and high credit utilization.\nStats: Range (medium - max).\n\nCredit Utilization - 65% - 97% (High)\nCash Advance - $3,431 - $11,221 (High)\nPurchases - $0 - $1,168 (Low)\nBalance - $2,701 - $9,560 (High)\n\nProduct Recommendations:\n\nBalance Transfer Card Options: Option to transfer their balance to 0% APR cards to save on interests rates.\nDebt Consolidation / Personal Loans: Provide debt consolidation loans with lower interest rates, allowing these customers to combine their high-interest debts into a single manageable payment, thus reducing their interest charges and simplifying their repayment strategy.\nCredit Counselling: Provide guidance on managing debt, creating budgets and financial improvements.\nCredit Builder Loans: Credit builder loans are specifically designed to help individuals build/improve their credit scores. Such loans are secured by the borrower’s savings account and timely payments are reported to the credit bureaus. These loans benefit both the financial institution through interest earned and also benfit the customer by establishing/rebuiliding positive credit history.\n\n\nCluster 2: Purchasing Customer\n\nDescription: These customers use their credit cards as a means for making frequent purchases. This may be due to the convenience and rewards earned by using their credit cards.\nStats: Range (medium - max).\n\nCredit Utilization - 35% - 93% (Medium to High)\nCash Advance - $49 - $4,161 (Low to High, excluding outliers)\nPurchases - $1,813 - $4,924 (High)\nBalance - $365 - $4,279 (Low to Medium)\n\nProduct Recommendations:\n\nRewards Programs: Reward programs where customers earn points, cashback, or airline miles based on their spending. These programs encourage customers to use their credit cards and provide incentive for loyalty.\nPremium Cards: This bank could offer premium cards with enhanced benefits for frequent users. While these cards typically come with higher annual fees (value for the bank), they also provide additional perks such as airport lounge access, concierge services, travel insurance, etc. (value for the customer).\nInstallment Plans: This bank could offer installment plans to these customers, enabling them to convert larger purchases in smaller, more manageable monthly payments.\nCredit Lines / Lower Interest Rates: Frequent card users who have established positive credit history could be eligible for increased credit lines and lower interest cards.\n\n\nCluster 3: Prudent Customer\n\nDescription: These customers take a cautious and thoughtful approach to their credit card usage. They have a sense of responsible financial behavior and consider their spending decisions carefully.\nStats: Range (medium - max).\n\nCredit Utilization - 4% - 52% (Low to Medium, excluding outliers)\nCash Advance - $0 - $1,465 (Low to Medium)\nPurchases - $460 - $3,431 (Low to Medium)\nBalance - $70 - $1,393 (Low to Medium)\n\nProduct Recommendations:\n\nLow Interest Cards: Offer credit cards with low annual percentage rates (APRs). These cards offer a lower cost of borrowing, allowing customers to manage their balances with minimal interest charges. Banks benefit from interest earned, while customers can maintain their cautious approach without worrying about excessive interest fees.\nSecured Credit Cards: Secured credit cards are an option for customers who want to build or rebuild their credit while maintaining control over their spending. These cards require a security deposit as collateral, which sets the credit limit. This offers an opportunity for cautious customers to establish a positive credit history while banks benefit from the security deposit and the potential to convert customers to unsecured credit cards in the future.\nFraud Protection and Monitoring: Banks can provide enhanced fraud protection and monitoring services for cautious customers. These services include real-time transaction alerts, identity theft protection, and advanced security features to safeguard against fraudulent activities. Customers gain peace of mind knowing that their accounts are closely monitored and protected, while banks can reduce the risk of financial losses due to fraud.\n\n\nCluster 1: Selective Customer\n\nDescription: These customers also have a careful and deliberate approach to credit card usage, primarily for low/medium value infrequent purchases. They are selective in their spending choices and prioritize making low/medium value purchases rather than frequent small transactions.\nStats: Range (medium - max).\n\nCredit Utilization - 13% - 98% (Low to High)\nCash Advance - $60 - $3,980 (Low to Medium)\nPurchases - $95 - $1,810 (Low to Medium)\nBalance - $365 - $4,279 (Low to Medium)\n\nProduct Recommendations:\n\nValue Purchase Financing: Banks can offer specialized financing options for infrequent purchases, allowing customers to pay for these purchases over time with low or 0% interest rates. This benefits customers by providing affordable installment plans and flexibility in managing their cash flow, while banks generate interest revenue and foster customer loyalty.\nExtended Warranty Protection: Banks can provide extended warranty protection as an added benefit for customers making certain infrequent high-value purchases. This extends the manufacturer’s warranty of these high-value purchases, offering coverage against unexpected repairs or replacements, providing peace of mind to customers. The bank benefits from increased card usage and customer satisfaction, potentially leading to long-term loyalty.\nPersonalized Spending Insights: Banks can offer personalized spending insights and analysis for customers who make infrequent purchases. This includes detailed transaction categorization, spending trends, and recommendations tailored to their unique spending patterns. It benefits customers by helping them make informed financial decisions and optimize their spending, while banks enhance customer engagement and strengthen relationships by providing valuable financial insights.\n\n\n\nThese are just a few examples of how this bank can build customer personas from the various customer segments/clusters and market financial products to them. It is important that these financial products are mutually beneficial to both the bank and the customers.\nThe bank can test the success of these financial products through various ways including experimentation (a/b testing), surveys, user analytics, net promoter score (NPS), pilot programs, etc."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#conclusion",
    "href": "projects/customer_segmentation_banking/index.html#conclusion",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, customer segmentation and persona creation using the k-means algorithm have emerged as powerful tools in the banking industry. By leveraging data analytics and machine learning techniques, banks can gain a deeper understanding of their customers, tailor their offerings to specific segments, and deliver personalized experiences. The k-means algorithm enables the identification of distinct customer groups based on behavioral patterns and characteristics, allowing banks to develop targeted strategies, design relevant financial products, and enhance customer satisfaction. With customer segmentation and persona creation at the core of their business strategies, banks can foster stronger customer relationships, drive growth, and stay ahead in today’s competitive landscape."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#data-prep",
    "href": "projects/customer_segmentation_banking/index.html#data-prep",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Data Prep",
    "text": "Data Prep\nTo prepare the dataset for k-means clustering, I performed the following steps:\n\nCreate an analysis cohort by filtering for customers with tenure = 10.\nDrop any rows with null values. This leaves an analysis cohort of 226 customers.\nCreate a credit_utilization feature by dividing balance by credit_limit."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#feature-engineering",
    "href": "projects/customer_segmentation_banking/index.html#feature-engineering",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nFeature engineering is an important step in preparing data for k-means clustering. I used the following techniques for feature engineering in order to improve the accuracy and effectiveness of the algorithm:\n\nRemove unwanted features - Remove the cust_id column. This is not needed for k-means and is used only as a customer identifier. I also make the executive decision to remove the installment_purchases column. This feature is defined as the “amount of purchases done in installments”. I would assume that purchases is the important feature in this dataset and regardless of the number of purchase installments.\nNormalization - This step scales each feature to have a mean of zero and standard deviation of one. This helps prevent features with larger values from dominating the clustering results.\nDimensionality Reduction (PCA) - Reducing the dimensionality of the data can reduce the computational complexity and improve the interpretability of the clustering results. A popular dimensionality reduction technique is called PCA (Principal Component Analysis). PCA works by identifying a smaller number of variables, known as principal components, that can explain the maximum amount of variance in the original dataset. These principal components are linear combinations of the original variables, and each one represents a different direction in the data. The first principal component captures the most variance in the data, while each subsequent component captures as much of the remaining variance as possible. For a video explanation of how PCA works, see this link.\n\nSetting a PCA threshold of 80%, meaning generating enough components to capture 80% of the variability in the features, PCA identified 5 principal components from 16 features. See sample below;"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#optimal-number-of-clusters",
    "href": "projects/customer_segmentation_banking/index.html#optimal-number-of-clusters",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Optimal Number of Clusters",
    "text": "Optimal Number of Clusters\nTo determine the optimal number of clusters, we can use a Skree Plot. A skree plot is used to determine the appropriate number of clusters to use in a k-means clustering analysis. The skree plot displays the within-cluster sum of squares (WCSS) on the y-axis, plotted against the number of clusters on the x-axis.\nThe within-cluster sum of squares measures the total squared distance between each point and it’s assigned cluster center, summed over all clusters. The scree plot helps identify the elbow point, or the point on the plot where adding more clusters does not significantly improve the clustering performance, i.e., the WCSS is not significantly reduced by adding more clusters.\nThe elbow point is typically used as a guide for deciding how many clusters to retain in the analysis. However, there is no hard and fast rule for selecting the number of clusters, this can be based on domain knowledge and knowledge about the dataset. The plot below is the skree plot for our analysis cohort;\n\n\n\n\n\nObservation: We can see that the elbow point appears to be 5 clusters. However after running the first iteration of the k-means algorithm, I noticed that only 1 customer (cust_id C11004) was being assigned to cluster 1. I would assume that this customer has certain characteristics that stand out for the rest of the cohort. For this particular analysis, I just decided to exclude this customer and re-cluster again, using 4 clusters."
  },
  {
    "objectID": "index.html#hi-im-lucas",
    "href": "index.html#hi-im-lucas",
    "title": "Lucas Okwudishu",
    "section": "",
    "text": "I am a data and analytics professional with over 10 years of experience in business operations and analytics. I have expertise in multiple industries including retail, manufacturing, non-profit and marketing.\nI have a proven track record of improving business operations and driving ROI true insights and action.\nI love digging into data to understand how various business components affect top-line revenue. This helps with better revenue modeling as well as understanding what levers can be used to drive revenue while eliminating inefficiencies."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Explore My Data Science Portfolio",
    "section": "",
    "text": "Discover how I harness the power of data and analytics through machine learning, predictive analytics, and insightful visualizations. Each project is a unique story of challenges met with innovative solutions, crafted to turn complex data into actionable insights.\n\n\n\n\n\n\nNote\n\n\n\nShorter excerpt of some of these projects are available on Medium\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning for Email Lead Scoring\n\n\nA Case Study of Predictive Analytics in Marketing\n\n\n\n\npredictive analytics\n\n\nmarketing analytics\n\n\nmachine learning\n\n\nroi analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nLucas Okwudishu\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced Predictive Analytics for Managing Overnight Shelter Capacity\n\n\nUnifying BigQuery DBT, H2o, and APIs for Social Good\n\n\n\n\npredictive analytics\n\n\nmachine learning\n\n\ndata modeling\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nLucas Okwudishu\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Hourly Bike Rentals\n\n\nIn Persuit of Model Accuracy\n\n\n\n\npredictive analytics\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nLucas Okwudishu\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Segmentation & Persona Creation in the Banking Industry\n\n\nA Case Study Using K-Means Algorithm\n\n\n\n\ncustomer segmentation\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nLucas Okwudishu\n\n\n\n\n\n\n  \n\n\n\n\nStrategic Bike Pricing: Harnessing Machine Learning for Market Edge\n\n\nAn Analytic Approach to Competitive Price Modeling in Bike Manufacturing\n\n\n\n\nmachine learning\n\n\nweb scraping\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\nLucas Okwudishu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/toronto_shelter_project/index.html",
    "href": "projects/toronto_shelter_project/index.html",
    "title": "Advanced Predictive Analytics for Managing Overnight Shelter Capacity",
    "section": "",
    "text": "Motivation\nMy motivation for this project comes from a desire to bridge the gap between academic exercises and real-world applications in data science. While my previous projects involved use of static data in CSV files, I recognize the need to evolve beyond this static framework. Moving forward, my goal is to build and maintain projects using data pipelines that can handle the ebb and flow of real-world changing/updating data. This project is a step towards that ambition.\n\n\nIntroduction\nIn the dynamic urban landscape of Toronto, overnight shelters represent more than just a temporary haven for the homeless. These facilities are a vital component of the city’s social support network, providing safety and stability for individuals and families facing the harsh realities of urban life. As an essential service, these shelters embody the collective effort to uphold the welfare of all citizens, ensuring that the city’s commitment to caring for its most vulnerable members is not just a promise, but a reality.\n\n\nProblem Statement\nThe challenge lies in effectively forecasting the occupancy rates of these shelters. Accurate predictions are not merely statistical triumphs, they are an important tool in the complex machinery of social support, determining the operational efficacy and resource optimization of these critical infrastructures. The implications of such forecasts extend beyond the confines of the shelters themselves, influencing policy-making, urban planning, and emergency response initiatives. The margin for error is narrow: underestimate, and we risk leaving individuals or families exposed to the elements. Overestimate, and we divert precious resources that could be utilized more effectively elsewhere. The quest for a reliable prediction model is not just a technical endeavor, it is a moral imperative to ensure that the well being of those in need is safeguarded.\n\n\nSolution Strategy\nThe proposed solution leverages the power of machine learning to analyze historical shelter occupancy data and generate accurate forecasts for overnight shelter occupancy. The approach is to train machine learning models that can identify patterns and trends from past data, considering various factors such as location, day of the week, month of the year, weather conditions, etc. By feeding the models with historical data, they learn to anticipate future demands with a considerable degree of accuracy. This predictive capability is enhanced by incorporating advanced algorithms and ensemble methods that can adapt to the changing dynamics of urban life. The end goal is a robust model capable of aiding shelter administrators to anticipate nightly occupancy rates and thus manage their operations with unprecedented foresight and efficiency.\n\n\nImplementation Strategy\n\n\n\n\n\n\nData Collection and Management:\n\nUtilize the Toronto Open Data API to collect historical occupancy data from multiple overnight shelters.\nGather additional relevant datasets such as historical weather patterns from AccuWeather API.\n\nData Storage:\n\nStore the collected data securely in Google BigQuery, ensuring the database scales with the inflow of new data and is optimized for complex analytical queries.\n\nData Transformation:\n\nUtilize DBT (Data Build Tool) to perform and manage necessary data transformations, ensuring that the data is clean, structured, and ready for model consumption.\n\nMachine Learning Model Development:\n\nUtilize H2o AutoML, an automated machine learning platform, to build and train predictive models.\nEvaluate various models and their performances, tuning hyper-parameters and selecting the best-performing model for deployment.\n\nDeployment:\n\nDeploy the trained machine learning model via a Shiny app.\nImplement robust app features that allow for real-time updates and the monitoring of model performance over time.\n\n\n\n\n\n\n\n\nShiny App\n\n\n\nYou can interact with the shiny app here.\n\n\nBy following this implementation strategy, this project aims to harness the vast potential of ML to deliver a solution that not only serves the immediate needs of overnight shelters in Toronto but also sets a standard for data-driven decision-making in humanitarian efforts city-wide.\nThe following sections walk through each step of the implementation strategy in more detail.\n\n\n\nData Collection\nThe first phase of the project involves gathering historical raw data on overnight shelter occupancy using Open Data APIs, which is then uploaded to BigQuery using an R script. This includes a one-time data upload for historical data from 2022 and ongoing daily updates for 2023. To enhance our prediction model, we also collect five-day weather forecasts for the Toronto area from the AccuWeather API. This weather information is a key factor in predicting shelter needs over the next five days, along with the occupancy data.\n\n\n\nData Storage\nAs mentioned earlier, this raw data is stored in a BigQuery project, in a dataset called “raw_data”. This dataset is also used to store historical daily weather data which is used as part of features for predicting overnight shelter occupancy.\n\n\n\nData Transformation\nThe data transformation phase is critical to ensuring data quality and readiness for subsequent analysis and predictive modeling. Leveraging DBT, a set of data models are created to convert raw datasets into a refined form and stored in a dataset named clean_data.\n\n\n\nFigure 1: Sample DBT model lineage\n\n\nBelow is an explanation of what each DBT model does;\n\nData Cleaning with DBT Models:\n\nshelter_occupancy_2022_2023: This model consolidates raw shelter occupancy records from 2022 and 2023. It performs a pivot transformation, changing the dataset from a wide format (with many columns) to a long format (with rows representing each data point), facilitating easier manipulation and analysis.\nshelter_occupancy_id_mapping: To standardize and uniquely identify characteristics within the data, this model assigns IDs to distinguishable attributes like program model and sector. This process aids in maintaining data integrity and consistency.\nshelter_occupancy_2022_2023_flagged_id: Building on the prior model, this model crafts unique primary keys that map each data row to a specific shelter location and program offering, thereby enabling precise tracking and referencing within the dataset.\nweather_historical_2022_2023: Here, historical weather information for 2022 and 2023 is merged.\n\nPreparing Data for Machine Learning:\n\nshelter_occupancy_weather_integration_2022_2023: This model integrates daily shelter occupancy figures with corresponding weather metrics for each particular date. It ensures completeness by employing a 3-day rolling average to impute any gaps in the weather dataset, thereby maintaining the continuity and reliability of the data for machine learning purposes.\n\nFeature Engineering for Predictive Modeling:\n\nshelter_occupancy_forecast_features: Targeting predictive analytics, this model generates forward-looking features essential for forecasting shelter occupancy. It projects the next five days’ dates for each shelter and program pairing and merges in the weather forecast for the same horizon. The result is a feature-rich dataset, crafted to anticipate future occupancy rates.\n\n\n\n\n\nMachine Learning & Model Development\nIn this phase, data is imported from BigQuery into R where machine learning models are built to predict overnight shelter occupancy. A few caveats in this phase -\n\nIn this beta phase, I focused on creating predictions for the top 3 organizations with the highest number of locations. These include City of Toronto (Org ID 1), Homes First Society (Org ID 15) and Dixon Hall (Org ID 6). All together, these 3 organizations have about 40 distinct locations. Again in a later phase, I hope to expand this project to include all locations.\nI also focused on only modeling locations that have been open (in 2022 and 2023) for at least 75% of the year.\nAccording to the information in the shelter occupancy dataset, there are two measures of capacity; Funding capacity reports the number of beds or rooms that a program is intended to provide. Actual capacity reports the number of beds or rooms in service and showing as available for occupancy in the shelter management information system at the time of reporting. There are a number of reasons why beds or rooms may be temporarily out of service, including maintenance, repairs, renovations, etc. Thus for making future predictions, a 7 day average of Actual capacity was used to estimate actual capacity. Also if the model’s prediction of overnight shelter occupancy for beds or rooms (occupied) exceeded the actual capacity, the occupied value was adjusted down to the actual capacity value. For example if the actual capacity for beds in a particular location was estimated to be 70, and the model predicted occupied beds for this location on a given night to be 73, this predicted value of 73 was adjusted down to 70 (to match the actual capacity), and thus resulting in predicted occupancy rate of 100%.\n\nAs mentioned earlier, H2o AutoML was used to create machine learning models. The model currently being used for making predictions is the H2o AutoML leader (StackedEnsemble_BestOfFamily_1_AutoML_2_20231026_61754). A Stacked Ensemble is a model that combines the strengths of multiple algorithms to make predictions. Looking at the metrics, the training data reflects an extremely promising model with very low error rates; MSE of 0.0648, RMSE of 0.2554, MAE of 0.1737.\n\n\n\nFigure 2: H2o AutoML Leaderboard\n\n\nHowever, a markedly different picture is painted when we consider the validation data metrics, which reveal an MSE of 152.8 and an RMSE of 12.3630. The MAE on the validation set is 7.1667. The large discrepancy between the training and validation metrics raises concerns about the model’s ability to generalize, suggesting that it may have over-fitted to the training data.\nCross-validation metrics serve as a middle ground, offering a more generalized assessment of model performance. These are less optimistic than the training metrics but far better than the validation metrics. With a cross-validated RMSE of 0.9291 and MAE of 0.3556, the model demonstrates decent predictive ability on data it was not trained on. However, the higher standard deviation in these cross-validated metrics suggests some variability in the model’s performance across different subsets of the data.\nIt is essential to note that discrepancies between training and validation metrics, such as those observed, could potentially signal data leakage, overfitting, or issues with the data splits, all of which are aspects of the pipeline that require careful attention. However, such concerns will be addressed in the subsequent iterations of the project, where model accuracy and generalization become the central focus.\n\n\n\n\n\n\nNote\n\n\n\nThe immediate goal is to establish that the pipeline correctly processes data through all its stages, ensuring that the model receives the input in the expected format and that the output is appropriately captured and stored. This verification is crucial for scalability and reliability before fine-tuning the model’s predictive capabilities. Once this is confirmed, iterative improvements on the model, such as hyperparameter tuning, cross-validation strategy adjustments, and feature selection, will be undertaken to optimize the model for accuracy and robustness.\n\n\n\n\n\nPredictions\nBefore making predictions, an essential data preparation step is conducted using a DBT model named shelter_occupancy_forecast_features. This DBT model is responsible for transforming raw data into a structured format that can be utilized for making predictions, essentially generating features that will feed into the stacked ensemble model. The step-by-step process is described below;\n\nFeature Engineering in BigQuery: The raw data is processed by the DBT model within BigQuery, The shelter_occupancy_forecast_features model applies necessary transformations and engineering to prepare the dataset with features suitable for the predictive model.\nData Import into R: Once the features are ready, the processed dataset is imported into an R environment.\nPrediction with the Stacked Ensemble Model: Within R, the stacked ensemble model is applied to the feature set to generate forecasts. Given the model’s composition from the AutoML run, it utilizes its combination of base learners and meta-learner to output highly informed predictions.\nExport Predictions to BigQuery: The predictions made by the ensemble model are then exported back into BigQuery. This reintegration allows for seamless storage and potential further analysis within the cloud data warehouse environment.\n\nThis pipeline exemplifies a modern analytical workflow, leveraging the strengths of various technologies; BigQuery for scalable data storage and preprocessing, DBT for data transformation and feature engineering, R for statistical modeling and predictions, and H2O for advanced machine learning techniques. By loading the predictive results back into BigQuery, the data becomes readily accessible for business intelligence tools, further analytics, or downstream applications, ensuring a cohesive and continuous flow of data from raw inputs to actionable outputs.\n\n\n\nDeployment\n\n\n\n\n\nThe deployment of the shelter occupancy forecast into a Shiny application represents the culmination of the data pipeline and the predictive modeling process. Shiny is a highly flexible R package that enables the creation of interactive web applications directly from R. Here’s how the final stage unfolds:\n\nImporting Predictions: The occupancy predictions, which have been computed by the H2o Stacked Ensemble model and stored in BigQuery, are imported into the Shiny application environment.\nInteractive User Interface: The Shiny application is developed with a user-friendly interface that presents the forecast data to end-users. It will feature:\n\nData Table: An interactive table displaying the forecasted overnight shelter occupancy across multiple locations and programs. This table allows for sorting, searching, and perhaps filtering to enable users to easily navigate through the data.\nMap Visualization: A map component will visualize the different shelter locations. The map will be color-coded based on predicted occupancy rates, providing an intuitive, at-a-glance understanding of the data. Locations can be marked with pins or regions shaded to reflect the predicted occupancy, giving users a geographical context.\n\nUser Interaction and Customization: Users will have the ability to select different dates, locations, or other relevant parameters to refine the predictions displayed. The Shiny app’s reactivity will ensure that any changes in user input will automatically update the visualizations and data presented.\n\nThis Shiny application will serve as a decision support tool, enabling stakeholders and decision-makers to access predictive insights in a user-friendly format. It provides a practical means for users to explore and utilize the model’s forecasts to manage shelter occupancy more effectively, ultimately aiding in resource planning and allocation for the locations and programs in question.\n\n\n\nConclusion\nThis project highlights the potential of predictive analytics in the realm of social good. By predicting shelter occupancy rates with greater accuracy, this kind of analysis not only helps cities optimize resource allocation but also serves as a beacon of hope, ensuring that shelters can prepare for and meet the needs of the most vulnerable members of society. It’s a vivid illustration of how data, technology, and human compassion can intersect to create a profound and positive social impact.\nAdditionally, this project demonstrates the practical implementation of a real-world analytics pipeline, from data collection to the complexities of model deployment. By leveraging public APIs, coupled with the analytical capabilities of R, the robust storage of BigQuery, and the power of DBT for data modeling, we’ve demonstrated a successful proof of concept.\nFinally, this project also demonstrates some key competencies including business understanding that guides the project’s direction, data gathering and working with APIs, sophisticated data engineering to sculpt raw information into actionable insights, advanced machine learning to forecast outcomes accurately, and effective model deployment to bring these insights into operational use.\n\n\n\nAdditional Next Steps\nThere are still a few things that could enhance this project as well as some improvements that could be made to the current workflow;\n\nAccuracy Tracking:\n\nScript for Tracking Predictions vs. Actuals: Develop and integrate a script within the R environment that routinely compares the predicted values of overnight shelter occupancy with the actual figures as they become available. Include statistical measures such as MAE, RMSE to compute accuracy of predictions in production.\nAlerts for Deviations: Implement an alert system that notifies the team when the discrepancy between predicted and actual values exceeds a predefined threshold, indicating a potential drift in model performance or significant changes in the underlying data.\n\nScale Model Training and Predictions to all Locations:\n\nCurrently, a few selected locations are being used in this project. The end goal is to scale modeling and predictions to all locations.\n\nBetter Model Experiment Tracking:\n\nIncorporate model tracking and experiment logging capabilities using a tool like MLflow.\n\nEnhance Shiny Application Enhancement with Modules:\n\nModular Design: Refactor the existing Shiny app into a modular design. Shiny modules are reusable parts of a Shiny app that can simplify app development and maintenance. They help in organizing the code, especially for complex applications with multiple reactive outputs and inputs.\n\n\n\n\nReproducible Code\n\nR Project.\nDBT Project."
  },
  {
    "objectID": "projects/bike_rental_prediction/index.html",
    "href": "projects/bike_rental_prediction/index.html",
    "title": "Predicting Hourly Bike Rentals",
    "section": "",
    "text": "A data science project to predict hourly bike rentals in the city of Seoul using date and weather seasons. Read analysis write up on Medium"
  },
  {
    "objectID": "projects/bike_price_recommender/index.html",
    "href": "projects/bike_price_recommender/index.html",
    "title": "Strategic Bike Pricing: Harnessing Machine Learning for Market Edge",
    "section": "",
    "text": "In the competitive bike manufacturing industry, pricing strategies can significantly impact market share and profitability. A hypothetical bike manufacturer faces the challenge of setting prices that are competitive yet profitable, adapting to market demands without compromising margins. To stay competitive, the manufacturer needs a data-driven approach to dynamically adjust prices based on bike features and market trends."
  },
  {
    "objectID": "projects/bike_price_recommender/index.html#implementation",
    "href": "projects/bike_price_recommender/index.html#implementation",
    "title": "Strategic Bike Pricing: Harnessing Machine Learning for Market Edge",
    "section": "Implementation",
    "text": "Implementation\n\nData Collection: Data was gathered through web scraping from the Trek bikes website, ensuring a rich dataset for analysis.\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\nWeb-scraping script available here.\n\n\n\nFeature Engineering: Bike attributes such as model, year, frame material, and additional features like electrical components and shock presence were encoded. Keywords indicative of special features and parts manufacturers were also extracted to enrich the dataset.\n\n\n\n\n\n\n\nData Prep\n\n\n\nData prep script available here.\n\n\n\nModel Development: Four machine learning models were chosen for price prediction: RANDOM FOREST, XGBOOST, GLMNET (Linear Regression), and MARS (Multivariate Adaptive Regression Splines). Model Training and Validation: Each model was trained using the prepared dataset, validated, and its pricing predictions were compared to actual prices to gauge competitiveness.\nStrategy Formulation: The RANDOM FOREST model was designated as the standard “every day” pricing model due to its accurate price predictions. The XGBOOST model was chosen for price markups given its higher price points, while the GLMNET and MARS models were selected for markdown scenarios due to their lower price predictions. Deployment:\n\n\n\n\n\n\n\nModeling\n\n\n\nModeling script available here.\n\n\n\nDeployment: A user-friendly shiny app was developed to deploy the analysis. The app allows users to select bike specifications and select a pricing model to receive price recommendations.\n\n\n\n\n\n\n\nShiny App\n\n\n\nCode for shiny app available here."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Coming Soon…\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/email_lead_scoring/index.html",
    "href": "projects/email_lead_scoring/index.html",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "",
    "text": "Businesses face challenges in identifying and prioritizing potential customers and/or identifying the future purchase potential of current customers based on their email interactions, leading to sub-optimal allocation of resources and missed opportunities.\nEmail lead scoring plays a crucial role in determining the quality and conversion potential of leads generated through email marketing campaigns. Email lead scoring is a method used by marketers and sales teams to evaluate and prioritize leads based on their potential to become customers. It involves assigning scores or ratings to individual leads based on their behavior, interactions, and other characteristics. However, the traditional manual lead scoring methods are time-consuming, subjective, and often produce inconsistent results. Additionally, these methods do not fully leverage the available data, such as email content, sender information, and historical customer interactions.\nMachine learning based solutions can effectively evaluate the probability of leads converting into customers based on various data points extracted from email interactions. This solution should take into account factors like email open rates, click-through rates, response times, engagement patterns, and historical customer data to provide a comprehensive lead score.\nBy leveraging machine learning algorithms, such as classification models businesses can create a reliable and automated system that can accurately score and rank email leads according to their conversion potential. The solution will empower email marketers to prioritize their efforts and resources more effectively, enabling them to focus on the most promising leads and improve overall sales and marketing efficiency.\nNote: This project was completed as part of the Python for Machine Learning and APIs course by Business Science University.\n\n\n\n\n\nThe project demonstrates and end-to-end email lead scoring solution for a business, from initial analysis, to modeling, to deployment. Skills demonstrated in this project include -\n\nProject management. ✅\nBusiness understanding. ✅\nBusiness return on investment and sensitivity analysis. ✅\nExploratory data analysis. ✅\nMachine learning (using tools like python, pycaret, mlflow). ✅\nModel deployment (using tools like fastapi and streamlit). ✅\n\nAbove all, the project demonstrates how to solve key business problems in the real world."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#objective",
    "href": "projects/email_lead_scoring/index.html#objective",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "2.1 Objective",
    "text": "2.1 Objective\nGiven these key insights, the problem at hand is to develop an effective email list scoring and segmentation strategy. The goal is to identify and prioritize the most valuable leads (in terms of probability of making a purchase) to target for sales emails, while also identifying leads with a low probability of purchase to nurture and increase their likelihood to purchase.\nIn summary, the primary objective is to leverage email list scoring and segmentation techniques to improve customer engagement, reduce unsubscribes, increase customer conversion rates, and ultimately maximize revenue and customer lifetime value.\nNow that we have a general understanding of the problem statement and the objective, the next sections will focus on a business solution process using the Business Science Problem Framework."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#what-is-the-business-science-problem-framework",
    "href": "projects/email_lead_scoring/index.html#what-is-the-business-science-problem-framework",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "2.2 What Is the Business Science Problem Framework",
    "text": "2.2 What Is the Business Science Problem Framework\n\n\n\n\n\nThe Business Science Problem Framework (BSPF) is a structured approach used for solving complex business problems, especially those involving data science and analytics. It’s designed to streamline the process of transforming a business problem into an analytical problem, and then into a data science solution."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#cost-assessment",
    "href": "projects/email_lead_scoring/index.html#cost-assessment",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "3.1 Cost Assessment",
    "text": "3.1 Cost Assessment\nGiven the values highlighted in the problem statement section, we can estimate the monthly lost revenue (we’ll refer to this as cost going forward) due to unsubscribes to be around $250K per month (or $3M annually), not factoring in email list growth rate. This hidden cost is shown in the table below -\n\n\n\n\n\nAfter factoring in a 3.5% monthly email list growth rate (last column), we can expect the lost revenue due to unsubscribes to rise to around $364K per month (or $4.3M per year), an increase of 46% in lost revenue.\nWe can see the high cost of this problem which is the lost revenue to the business. However, the values shown in the table above do not factor in uncertainty. We can thus improve on our cost assessment by factoring in uncertainty in some of the drivers. Let assume some monthly variability in email list growth rate and conversion rate. The heatmap below shows a cost simulation with variability. The y-axis represents various levels of customer conversion rate while the x-axis represents various levels of email list growth rate;\n\n\n\n\n\nWe can see that regardless of how the drivers vary, we can still expect to see annual costs ranging from $2.61M to $4.38M. Thus this is definitely a problem worth solving.\nAt this point, a key question is “can we reduce the unsubscribe rate?”. Recall that the business is losing 500 subscribers for every email sent out. What if we can reduce that number by 50% (or 250), while maintaining 90% of revenue. What impact will that have on the business?\nThe only problem is that we still do not know a lot about what causes a subscriber to make a purchase. If we do, we can focus on targeting the ones that are more likely to purchase with sales emails and nurture the ones who are unlikely to covert at the moment. This will help with our goal of reducing the unsubscribe rate while maintaining 90% of revenue."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#exploratory-data-analysis",
    "href": "projects/email_lead_scoring/index.html#exploratory-data-analysis",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "4.1 Exploratory Data Analysis",
    "text": "4.1 Exploratory Data Analysis\nAfter some initial data manipulation to get the data in the right format we need for analysis, including adding a target feature which is a binary flag of if a lead has made a previous purchase or not (we’ll call this target feature made_purchase), we can then begin to do some initial exploratory analysis to get a sense what features in our dataset could be predictive of leads making a purchase.\nFirst, lets analyze our target feature made_purchase.\n\n4.1.1 Proportion of Subscribers with Previous Purchase\n\n\n\n\n\nObservation: Only 5% of leads have made a previous purchase, meaning we are dealing with a highly imbalanced dataset.\n\n\n4.1.2 Member Rating vs Made Purchase\nThe plot below shows the different values of member_rating vs the proportion of made_purchase for users with each value of member_rating -\n\n\n\n\n\nObservation: member_rating appears to be quite predictive of made_purchase. We can see that the likelihood of a user making a purchase increases 3x when the member_rating goes from 1 to 2. Additionally the likelihood of making a purchase increases 5x when the member_rating goes from 2 to 5.\n\n\n4.1.3 Country Code vs Made Purchase\nThe plot below shows country_code along with the within group made_purchase proportion, for the top 10 countries in terms of count of users -\n\n\n\n\n\nObservation: country_code also appears to be quite predictive of made_purchase as well.For example we can see that while the US has the most users (over 3,500), the proportion of US users who have made a purchase is ~10%. However a country like AU, which has less than 500 users has a higher proportion of users who have made a purchase (~12%).\n\n\n4.1.4 Tag Count vs Made Purchase\nThe plot below shows tag_count along with the within group made_purchase proportion, for a selected number of tag_count. Recall that tags here refer to various events like webinars. Therefore tag_count refers to the number of such events a user has attended -\n\n\n\n\n\nObservation: We can see that if a subscriber has 40 or more tags (events), they are 100% likely to make a purchase. That likelihood drops as tag_count decreases. Note that a lead with 0 tags only have a 2% likelihood of making a purchase. For those with 0 tags (meaning they have not attended any events yet) we may not want to send them emails just yet. We may want to try and nurture them to attend more events before trying to get them to make a purchase. Overall if the business can get leads to attend more events, it drastically increases their likelihood of making a purchase.\n\n\n4.1.5 Correlation\nThe plot below is a correlation heatmap of numeric features only -\n\n\n\n\n\nObservation: These correlation values further validate some of the data we saw earlier. We can see that tag_count and member_rating do show a fairly high correlation with made_purchase."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#testing-multiple-models",
    "href": "projects/email_lead_scoring/index.html#testing-multiple-models",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "7.1 Testing Multiple Models",
    "text": "7.1 Testing Multiple Models\nSeveral models were initially tested, using Area Under the Curve (AUC) as the key metric. Higher AUC indicates a better-performing model in distinguishing positive and negative leads. The chart below shows the AUC along with other metrics from initial modeling. We can see the top 3 models in terms of AUC are Gradient Boosting Classifier (0.8044) CatBoost Classifier (0.8015) and Ada Boost Classifier (0.7965)."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#model-metrics",
    "href": "projects/email_lead_scoring/index.html#model-metrics",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "7.2 Model Metrics",
    "text": "7.2 Model Metrics\nHere we can examine metrics for the Catboost model.\n\n\n\n\n\n\nModel Metrics (Catboost)\n\n\n\nThe choice of Catboost, a tree-based model, is driven by its inherent advantage in explainability, which is critical for understanding model decisions and behavior. It’s important to note that the final model selection will be determined in the Return on Investment (ROI) section of the course. However, it is worth mentioning that all the models under consideration exhibit very similar performance metrics. Consequently, regardless of the final choice, we can anticipate that the selected model will demonstrate metrics comparable to those of the Catboost model. This consistency across models ensures reliability in the anticipated performance, irrespective of the specific model chosen.\n\n\n\n7.2.1 AUC-ROC Plot\nAn AUC-ROC (Area Under the Receiver Operating Characteristic Curve) plot is a graphical representation that summarizes the performance of a binary classification model. It showcases the model’s ability to distinguish between positive and negative instances by plotting the true positive rate against the false positive rate. The AUC value, ranging from 0 to 1, quantifies the model’s accuracy, with a higher value (higher curve towards 1) indicating better performance. The AUC-ROC plot enables data scientists and decision-makers to assess and compare models, aiding in the selection and optimization of classification algorithms.\n\n\n\n\n\n\n\n7.2.2 Confusion Matrix\nThe confusion matrix is a tabular representation that provides a comprehensive summary of the performance of a classification model. It organizes predictions made by the model into four categories: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nTrue Positive (TP): The model correctly predicts the positive class.\nTrue Negative (TN): The model correctly predicts the negative class.\nFalse Positive (FP): The model incorrectly predicts the positive class when it should have been negative (Type I error).\nFalse Negative (FN): The model incorrectly predicts the negative class when it should have been positive (Type II error).\n\n\n\n\n\n\nLet’s understand the confusion matrix above;\n\n24 predictions (bottom right) are true positives, these are the subscribers the model predicted will make a purchase and they did make a purchase. 27 (top right) are false positives. The model predicted them to make a purchase and they did not. This is where we might have wasted effort.\n168 (bottom left) are false negatives. The model predicted they will not make a purchase but they did. These are missed opportunities. 3765 (top left) are true negatives. The model predicted they will not make a purchase and they did not. There is no impact for these.\n\n\n\n7.2.3 Feature Importance\nA feature importance plot is a graphical representation that helps us understand the relative importance of different features or variables in a predictive model. It provides insights into what factors or variables have the most significant impact on the outcome or target variable. The plot below show feature importance for the Catboost model;\n\n\n\n\n\nYou can see that the model shows the most importance features to be optin_days, member_rating and country_code_US. Note that this order may vary for another model. Different machine learning models can have different ranking for feature importance due to their inherent characteristics and the Algorithms they employ to make predictions. Factors such as model architecture, algorithmic approach, feature interactions, model assumptions, all play a part in how a model ranks feature imporance.\n\n\n7.2.4 Shap Values\nA SHAP (SHapley Additive exPlanations) values plot is a visual representation that provides insights into the contribution of individual features to the predictions made by a machine learning model. It is based on Shapley values, a concept from cooperative game theory, which assigns a value to each feature by measuring its impact on the prediction compared to its absence or average value. The SHAP values plot displays the magnitude and direction of each feature’s impact on the model predictions, allowing for a comprehensive understanding of how different features influence the outcomes. It helps identify which features have the most significant positive or negative influence on predictions and provides a clear picture of how the model is making decisions based on different feature values. This plot enables users, including business leaders, to interpret and explain the model’s behavior and make informed decisions based on the feature contributions.\nThe plot below shows shap values for the Catboost model;\n\n\n\n\n\nThe higher the shap value is (x-axis), the higher the likelihood of positive. For example we can see the higher shap values for member_rating and tag_count, meaning that subscribers who have higher values for these 2 features are more likely to predicted as making a purchase.\n\n\n7.2.5 Shap Values (Specific Observations)\nShap values can also be created for specific observations or individual leads in this case. Below is the plot of shap values for a customer with a predicted label of 0 and prediction score (probability) of 0.67.\n\n\n\n\n\nThe plot shows how the prediction of the model was influenced by each input feature, by displaying the contribution of each feature as a horizontal bar on the plot. The length of the bar represents the magnitude of each feature’s SHAP value, with longer bars indicating a larger impact on the prediction.\nThe color of the bars indicate the direction of the impact, with blue bars indicating a negative impact and red bars indicating a positive impact.\nIn this case, we can see that features like tag_count of 6, tag_learning_lab_22 and tag_learning_lab_2 positively impact the prediction while member_rating of 1 and country_code_IN negatively impact the prediction.\nIn contrast the plot below shows SHAP values for customer who also has a predicted label of 0 but a higher prediction score (probability) of 0.9;\n\n\n\n\n\nNotice that features that are positively impacting the score include country_code_other and tag_learning_lab_1 while features that are negatively impacting the score include country_code_IN and member_rating of 1.\nThese comparisons underscore the nuanced and individualized way in which different features contribute to the model’s predictions for each customer."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#blending-models-ensembling-for-enhanced-performance",
    "href": "projects/email_lead_scoring/index.html#blending-models-ensembling-for-enhanced-performance",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "7.3 Blending Models (Ensembling) for Enhanced Performance",
    "text": "7.3 Blending Models (Ensembling) for Enhanced Performance\nTo further improve the predictive performance and extract maximum value from our modeling efforts, we employed a technique called model ensembling. By combining the predictions of multiple models, we aimed to leverage the unique strengths of each model, resulting in a more robust and accurate ensemble prediction.\nAfter blending and calibrating the top three models, the resulting ensemble had an AUC of 0.8020 which is a very slight difference in AUC from the individual models;\n\nEnsemble AUC of 0.8020 vs Individual Models\n\n\n\nGBM\nCatboost\nAda Boost\n\n\n\n\nModel AUC\n0.8044\n0.8015\n0.7965\n\n\nVS Ensemble\n- 30%\n+ 0.06%\n+ 0.69%\n\n\n\nThis ensemble method showcases how combining individual models can enhance overall predictive accuracy, particularly in complex tasks like lead scoring.\nIn conclusion, this section provided a comprehensive analysis of our model metrics and interpretation. These metrics allow us to evaluate the performance and effectiveness of different machine learning models in predicting our target variable. Understanding these metrics is crucial for assessing the model’s overall predictive power and ensuring its reliability for decision-making. It is important to note that the significance of these model metrics will be revisited and tied to the return on investment (ROI) analysis in the subsequent sections of this project. By aligning the model’s performance with the business objectives and financial outcomes, we can gain deeper insights into the practical value and impact of the models deployed."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#what-is-mlflow",
    "href": "projects/email_lead_scoring/index.html#what-is-mlflow",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "8.1 What is MLflow?",
    "text": "8.1 What is MLflow?\nMLflow is an open-source platform designed to help data scientists and machine learning engineers track and manage their machine learning experiments. It provides tools for experiment logging, reproducibility, and model management. Developed by Databricks, MLflow aims to simplify the machine learning life-cycle by enabling users to keep track of experiments, compare different models, and efficiently share and deploy ML projects.\n\n\n\nEmail Lead Scoring MLflow Dashboard\n\n\nMLflow plays a critical role in the machine learning workflow, addressing several key challenges that data scientists often encounter:\nExperiment Tracking: MLflow allows data scientists to log their experiments with ease. This includes recording the hyperparameters, metrics, and model artifacts associated with each run. Such comprehensive tracking facilitates comparison between different model iterations and helps in selecting the best performing model for deployment.\nReproducibility: In machine learning, it is crucial to ensure that experiments can be reproduced with the same results. MLflow records the exact versions of libraries, data, and code used in each run, making it easier to replicate the experiments and maintain consistency across different environments.\nCollaboration: In team-based data science projects, collaboration is essential. MLflow enables seamless sharing of experiments, models, and associated artifacts with colleagues, promoting knowledge sharing and fostering a collaborative environment.\nModel Management: MLflow provides functionalities for model versioning and management. This allows data scientists to keep track of model iterations, deploy the best-performing models, and roll back to previous versions if needed."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#integrating-mlflow-with-pycaret",
    "href": "projects/email_lead_scoring/index.html#integrating-mlflow-with-pycaret",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "8.2 Integrating MLflow with PyCaret",
    "text": "8.2 Integrating MLflow with PyCaret\nPyCaret, can be seamlessly integrated with MLflow to leverage its powerful experiment tracking capabilities. The integration allows data scientists using PyCaret to log their experiments automatically into MLflow, making it easy to keep track of multiple experiments and compare different models efficiently.\n\n\n\nFinalized Model Details\n\n\nIn this project, MLflow will be a valuable asset as the project progresses and our machine learning models go from development to production. MLflow ensures that the lead scoring process remains organized, reproducible, and collaborative. Its tracking, versioning, and management capabilities contribute to the success and efficiency of the lead scoring system, making MLflow an indispensable component in the data science workflow."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#initial-threshold-cost-savings",
    "href": "projects/email_lead_scoring/index.html#initial-threshold-cost-savings",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "9.1 Initial Threshold & Cost Savings",
    "text": "9.1 Initial Threshold & Cost Savings\nThis step involved calculating the cumulative gain of our machine learning model to enhance our return on investment (ROI). Here the predictions are sorted based on the model’s score (probability) of making a purchase. The Cumulative Gain then measures the proportion of Hot-Leads and Cold-Leads based on a threshold. An arbitrary threshold of 0.95 is used here as the cut off for categorizing Hot-Leads and Cold-Leads.\nThe table below shows the proportion of hot and cold leads based on this arbitrary 0.95 threshold.\n\nHot / Cold Lead Categorization (95% Gain Threshold)\n\n\nCategory\nTotal Leads\nMade Purchase\n\n\n\n\nCold-Lead\n5,528\n49\n\n\nHot-Lead\n14,691\n913\n\n\n\nThis approach refines our marketing strategy. Instead of targeting all leads with sales emails, we now only focus on sending sales emails to Hot-Leads, while simultaneously nurturing Cold-Leads, whom we’ll expect to make a purchase within 90 days. The result of this would be some initial loss in sales as not all leads are targeted. Based on the Cold-Lead row in the table above, we can see that 5528 (27%) of leads will NOT be targeted for sales emails, resulting in potential lost purchases of 49. However, the initial loss in sales will be offset as we nurture cold leads and get them to make a purchase within 90 days.\n\n9.1.1 Expected Value (Cost vs Savings)\nAs highlighted above, there is as cost and savings trade-off from only targeting Hot-Leads while only nurturing Cold-Leads. Expected Value in this case, is the financial value associated with various cost and saving scenarios. Let’s deep dive into this expected value calculations for this project, based on our threshold of 0.9 for determining hot and cold leads and the proportion of hot and cold leaders shown the previous table.\nBefore proceeding, we’ll also need some of the values stated earlier in the Business Understanding section -\n\nCurrent Business KPIs\n\n\nBusiness KPIs\nValue\n\n\n\n\nEmail List Size\n100,000\n\n\nUnsubscribes Per Email\n500\n\n\nSales Emails Per Month\n5\n\n\nUnsubscribe Rate\n0.50%\n\n\nAvg Sales Per Month\n$250,000\n\n\nCustomer Conversion Rate\n5.00%\n\n\nAvg Customer CLV\n$2,000\n\n\nSales Per Email Sent\n$50,000\n\n\n\nThese business KPIs are then used along with subscriber counts from the initial threshold table to generate some preliminary expected value calulations.\n\nPreliminary EV Calculations (Based on 95% Threshold and Sample Factor of 5.0)\n\n\n\n\n\n\n\nKPI\nValue\nDescription\n\n\n\n\nMissed Purchase Ratio\n5.0%\nProportion of leads that did not make a purchase\n\n\nMade Purchase Ratio\n94.91%\nProportion of leads that made a purchase\n\n\nSavings - Cold Leads Not Targeted\n$65,616\nMonthly savings from cold leads not targeted\n\n\nCost - Cold Leads Missed Sales\n$12,734\nMissed sales from cold leads not targeted\n\n\nCost - Hot Leads that Unsubscribe\n$184,384\nMissed sales from hot leads that unsubscribe\n\n\nSavings - New Monthly Sales\n$237,266\nMonthly sales in the first month of new strategy\n\n\n\n\n\n\n\n\n\nExpected Value Calculations\n\n\n\nTo understand how these values are calculated, please refer to this excel sheet.\n\n\nFinally these preliminary calculations are then used to calculate expected value. The table below is a breakdown of cost vs savings for this new strategy vs our old strategy of targeting all leads with sales emails.\n\nExpected Value of Implementing New Strategy (Based on 95% Threhold and Sample Factor of 5.0)\n\n\n\n\n\n\n\nEV (with New Lead Scoring Strategy)\nValue\nDescription\n\n\n\n\nRevenue in First Month\n$237,266\nSales in first month with new strategy\n\n\nNew Revenue (Post Nurturing Cold Leads)\n$290,148\nExpected total sales after nurturing cold leads\n\n\nExpected Monthly Savings\n$52,882\nExpected monthly savings with new strategy\n\n\nExpected Saved Customers\n33\nExpected saved customers with new strategy\n\n\n\nKey Takeaways: The proposed new strategy, focusing only on Hot-Leads, is expected to generate sales of approximately $237,266 in the first month. This approach differs from the previous strategy where we targeted all leads with sales emails. Despite this initial month’s sales being 5% lower than our usual monthly sales of $250,000, we will save $52,882 each month by not sending sales emails to Cold-Leads. More importantly, by nurturing Cold-Leads effectively and converting them into buyers within 90 days, we anticipate a significant increase in sales. We project our net sales to reach $290,148, marking a 16% rise compared to our current monthly sales."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#threshold-optimization-profit-maximization",
    "href": "projects/email_lead_scoring/index.html#threshold-optimization-profit-maximization",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "9.2 Threshold Optimization & Profit Maximization",
    "text": "9.2 Threshold Optimization & Profit Maximization\nOur initial ROI analysis used a 95% threshold to decide whether leads are hot (likely to buy) or cold (less likely to buy). This section explores how changing this threshold affects our expected profits and cost savings.\nThe main aim is to find the sweet spot that maximizes profit. However, we must also consider management’s concerns. For example, using the 95% threshold, we saw a 5% drop in sales in the first month. Senior management may be hesitant about any strategy that reduces monthly sales, even if it could lead to higher sales later on.\nThe plot below shows our optimization results. On the x-axis, various threshold levels are plotted, ranging from 0 to 1. The y-axis corresponds to the expected value associated with each of these thresholds. A notable feature on the chart is a red dashed line, marking the 88% threshold. This specific threshold that categorizes leads into Hot or Cold. This is the point where the maximum expected value is realized - in this instance, $322,893.\nThis maximum expected value is achieved under the condition that a minimum of 87% of the usual monthly sales is preserved, even when implementing the new email lead scoring strategy. Essentially, the 87% figure acts as a benchmark, ensuring that the new strategy does not significantly erode monthly sales. We refer to this benchmark as the Monthly Sales Reduction Safeguard. It serves as a critical parameter, ensuring that while maximizing the expected value from lead scoring, the strategy also safeguards a significant portion of the current monthly sales, maintaining at least $219,595 in this scenario.\n\n\n\n\n\nHowever, management might consider this 87% safeguard to be too low. They may prefer a higher safeguard, for example, 90%. This means the new strategy should retain at least 90% of our monthly sales. The plot also shows how the expected value changes when we apply this 90% safeguard.\n\n\n\n\n\nNotice that with a 90% safeguard, the expected value drops to $317,695. However we see an increase in the current monthly sales retained, $224,532 in this case.\nIn conclusion the core of our decision-making process lies in striking a balance between two crucial objectives; achieving higher expected value in the long term vs maintaining current monthly sales levels in the short term. This trade-off presents a strategic challenge that all stakeholders must collectively navigate. The choice between prioritizing immediate sales stability and pursuing potentially greater profits down the line is pivotal. The optimization exercise shows the implications of various thresholds, but it ultimately falls to the collective agreement of all involved parties to determine the most suitable path forward. This decision will shape not only our immediate financial landscape but also our strategic direction in the foreseeable future."
  },
  {
    "objectID": "projects/email_lead_scoring/index.html#api",
    "href": "projects/email_lead_scoring/index.html#api",
    "title": "Machine Learning for Email Lead Scoring",
    "section": "10.1 API",
    "text": "10.1 API\nOur API, developed using FastAPI, features several endpoints, each serving a specific purpose;\n\nMain Endpoint (\"/\") This is the landing page of our API. It provides users with a welcoming interface and guides them to the API documentation. This endpoint is crucial for user orientation and ease of use.\nGet Email Subscribers (\"/get_email_subscribers\") This GET endpoint exposes our email subscriber data. When accessed, it returns the data with scored leads in JSON format. It’s vital for stakeholders to view and understand the current leads database.\nData Reception (\"/data\") A POST endpoint designed to receive data. Users can submit data in JSON format, which is then processed and stored. This endpoint is essential for updating our leads database with new information.\nLead Scoring Prediction (\"/predict\") This POST endpoint is the heart of our API. It accepts lead data and returns scored leads, using our proprietary lead scoring models. It enables the application of our predictive model to new or existing data for real-time lead scoring.\nLead Scoring Strategy Calculation (\"/calculate_lead_strategy\") Another POST endpoint, it calculates and optimizes lead scoring strategies based on various parameters (e.g., sales reduction safeguard, email list size). This is crucial for strategic decision-making and marketing optimization.\n\nAPIs play a pivotal role in the data science lifecycle, especially in the deployment phase. They enable the seamless integration of data science models into business processes, making predictive insights accessible and actionable for decision-makers. APIs facilitate real-time data processing and interaction, which is essential for dynamic and responsive business strategies."
  }
]