[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lucas Okwudishu",
    "section": "",
    "text": "Bio\n\nHi I’m Lucas!\nI am a data and analytics professional with over 10 years of experience in business operations and analytics. I have expertise in multiple industries including retail, manufacturing, non-profit and marketing.\nI have a proven track record of improving business operations and driving ROI true insights and action.\nI love digging into data to understand how various business components affect top-line revenue. This helps with better revenue modeling as well as understanding what levers can be used to drive revenue while eliminating inefficiencies.\n\n\n\n\nExperience & Education\n\nAs a Senior Data Analyst at HubSpot, I leverage data to refine and bolster our marketing strategies, covering all aspects of the customer journey from acquisition and retention to upgrade.\nMy professional journey has also included analytic roles at United Way Worldwide, DAP, and Whole Foods Market. The consistent application of data intelligence has been the cornerstone of my professional journey.\nI also hold a masters degree in Applied Economics from Johns Hopkins University, and a certificate in Business Analytics from George Washington University.\nPlease contact me for a copy of my full resume.\n\n\n\n\nOpen Source Contribution\nI am currently contributing to the development of Pytimetk, an open source python package for time series analysis."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html",
    "href": "projects/customer_segmentation_banking/index.html",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "",
    "text": "The banking industry struggles with understanding and meeting the diverse needs of customers, resulting in generic marketing approaches to financial product offerings that fall short of creating personalized experiences. To address this challenge, a data-driven solution is required to effectively segment customers and create targeted personas for improved customer engagement and satisfaction, thus leading to increased revenue for banks/financial institutions.\n\n\nCustomer segmentation can be achieved through the powerful k-means algorithm. By leveraging data analytics and machine learning techniques, financial institutions (banks & credit card companies) gain valuable insights into customer behavior, preferences, and characteristics. This enables the identification of distinct customer segments and the development of personalized offerings. This has several advantages including enhanced customer understanding, improved customer engagement with product offerings, a healthier customer base, and increased revenue for financial institutions.\n\n\n\nTo implement customer segmentation and using the k-means algorithm, financial institutions should follow these steps;\n\nData Collection and Integration: Gather customer data from various sources, including transaction records, demographic information, and customer interactions.\nPreprocessing and Feature Selection: Cleanse and preprocess the data, selecting relevant features for analysis, such as transaction frequency, average balance, age, and customer preferences.\nK-Means Clustering: Apply the k-means algorithm to segment customers based on similar behavioral patterns. Determine the optimal number of clusters and assign customers to their respective segments.\nPersona Creation: Develop personas for each customer segment, incorporating demographic information, behaviors, preferences, and goals. These personas should be representative of the segment’s characteristics and serve as a guide for targeted marketing strategies.\nImplementation and Iteration: Implement personalized marketing campaigns and track the performance of the tailored strategies. Continuously evaluate and refine the segments and personas based on customer feedback and evolving market dynamics.\n\nThe following pages, walk through a detailed case study of customer segmentation and persona creation using the k-means algorithm.\n\n\n\n\n\n\nNote\n\n\n\nShorter excerpt available on Medium"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#solution",
    "href": "projects/customer_segmentation_banking/index.html#solution",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "",
    "text": "Customer segmentation can be achieved through the powerful k-means algorithm. By leveraging data analytics and machine learning techniques, financial institutions (banks & credit card companies) gain valuable insights into customer behavior, preferences, and characteristics. This enables the identification of distinct customer segments and the development of personalized offerings. This has several advantages including enhanced customer understanding, improved customer engagement with product offerings, a healthier customer base, and increased revenue for financial institutions."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#implementation-strategy",
    "href": "projects/customer_segmentation_banking/index.html#implementation-strategy",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "",
    "text": "To implement customer segmentation and using the k-means algorithm, financial institutions should follow these steps;\n\nData Collection and Integration: Gather customer data from various sources, including transaction records, demographic information, and customer interactions.\nPreprocessing and Feature Selection: Cleanse and preprocess the data, selecting relevant features for analysis, such as transaction frequency, average balance, age, and customer preferences.\nK-Means Clustering: Apply the k-means algorithm to segment customers based on similar behavioral patterns. Determine the optimal number of clusters and assign customers to their respective segments.\nPersona Creation: Develop personas for each customer segment, incorporating demographic information, behaviors, preferences, and goals. These personas should be representative of the segment’s characteristics and serve as a guide for targeted marketing strategies.\nImplementation and Iteration: Implement personalized marketing campaigns and track the performance of the tailored strategies. Continuously evaluate and refine the segments and personas based on customer feedback and evolving market dynamics.\n\nThe following pages, walk through a detailed case study of customer segmentation and persona creation using the k-means algorithm.\n\n\n\n\n\n\nNote\n\n\n\nShorter excerpt available on Medium"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#k-means-algorithm",
    "href": "projects/customer_segmentation_banking/index.html#k-means-algorithm",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.1 K-Means Algorithm",
    "text": "5.1 K-Means Algorithm\nK-Means algorithm is a popular unsupervised machine learning technique used for clustering data. The algorithm works by dividing a set of data points into k clusters based on their similarity. Initially, k centroids are chosen randomly from the data points, and each data point is assigned to the nearest centroid. Then, the centroids are moved to the center of their respective clusters, and the data points are re-assigned to the nearest centroid. This process is repeated until the centroids no longer move or a maximum number of iterations is reached.\n\n\n\n\n\nThe k-means algorithm is widely used in various applications, such as customer segmentation, and anomaly detection. To learn more about the k-means algorithm, you can read up on the following sources:\n\nK-Means Clustering.\nBenefits & challenges of k-means for customer segmentation\nHow to use k-means clustering for customer segmentation\n\nBefore proceeding with k-means clustering, I decided to filter the dataset down to a particular cohort of customers, in order to ease computational time and ease of analyzing/visualizing clusters. I’ll be picking customers with tenure at 10 months for my analysis cohort. This cohort has 236 customers and makes up 2.6% of the dataset.\n\n5.1.1 Data Prep\nTo prepare the dataset for k-means clustering, I performed the following steps:\n\nCreate an analysis cohort by filtering for customers with tenure = 10.\nDrop any rows with null values. This leaves an analysis cohort of 226 customers.\nCreate a credit_utilization feature by dividing balance by credit_limit.\n\n\n\n5.1.2 Feature Engineering\nFeature engineering is an important step in preparing data for k-means clustering. I used the following techniques for feature engineering in order to improve the accuracy and effectiveness of the algorithm:\n\nRemove unwanted features - Remove the cust_id column. This is not needed for k-means and is used only as a customer identifier. I also make the executive decision to remove the installment_purchases column. This feature is defined as the “amount of purchases done in installments”. I would assume that purchases is the important feature in this dataset and regardless of the number of purchase installments.\nNormalization - This step scales each feature to have a mean of zero and standard deviation of one. This helps prevent features with larger values from dominating the clustering results.\nDimensionality Reduction (PCA) - Reducing the dimensionality of the data can reduce the computational complexity and improve the interpretability of the clustering results. A popular dimensionality reduction technique is called PCA (Principal Component Analysis). PCA works by identifying a smaller number of variables, known as principal components, that can explain the maximum amount of variance in the original dataset. These principal components are linear combinations of the original variables, and each one represents a different direction in the data. The first principal component captures the most variance in the data, while each subsequent component captures as much of the remaining variance as possible. For a video explanation of how PCA works, see this link.\n\nSetting a PCA threshold of 80%, meaning generating enough components to capture 80% of the variability in the features, PCA identified 5 principal components from 16 features. See sample below;\n\n\n\n\n\n\n\n5.1.3 Optimal Number of Clusters\nTo determine the optimal number of clusters, we can use a Skree Plot. A skree plot is used to determine the appropriate number of clusters to use in a k-means clustering analysis. The skree plot displays the within-cluster sum of squares (WCSS) on the y-axis, plotted against the number of clusters on the x-axis.\nThe within-cluster sum of squares measures the total squared distance between each point and it’s assigned cluster center, summed over all clusters. The scree plot helps identify the elbow point, or the point on the plot where adding more clusters does not significantly improve the clustering performance, i.e., the WCSS is not significantly reduced by adding more clusters.\nThe elbow point is typically used as a guide for deciding how many clusters to retain in the analysis. However, there is no hard and fast rule for selecting the number of clusters, this can be based on domain knowledge and knowledge about the dataset. The plot below is the skree plot for our analysis cohort;\n\n\n\n\n\nObservation: We can see that the elbow point appears to be 5 clusters. However after running the first iteration of the k-means algorithm, I noticed that only 1 customer (cust_id C11004) was being assigned to cluster 1. I would assume that this customer has certain characteristics that stand out for the rest of the cohort. For this particular analysis, I just decided to exclude this customer and re-cluster again, using 4 clusters."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#visualizing-clusters",
    "href": "projects/customer_segmentation_banking/index.html#visualizing-clusters",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.2 Visualizing Clusters",
    "text": "5.2 Visualizing Clusters\nUMAP (Uniform Manifold Approximation & Projection) is a machine learning technique for dimensionality reduction and data visualization. In order to visualize the cluster assignments, I used UMAP for the sole purpose of generating x and y axis coordinates for visualizing clusters. Recall our dataset for k-means was reduced to 5 principal components. I wanted to still be able to visualize the clusters in 2 dimensions.\nThe plot below shows our cluster assignments from the k-means algorithm;\n\n\n\n\n\nObservation: Although not perfect, the k-means algorithm does a decent job of segmenting customers. We can see 4 distinct segments with a few customers that should probably be re-clustered, although a 3 dimensional plot may present a different perspective."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#cluster-analysis",
    "href": "projects/customer_segmentation_banking/index.html#cluster-analysis",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.3 Cluster Analysis",
    "text": "5.3 Cluster Analysis\nNow that we have our clusters of customers, we can begin analyzing these clusters to understand trends in customer characteristics of the different clusters. Note that we have already seen some of these trends during the explanatory data analysis phase, thus we can better understand how the k-means algorithm is working. To start, we can plot the distribution of some features of interest by cluster using boxplots. Let’s start by plotting the distribution of balance, cash_advance, credit_utilization and purchases.\n\n\n\n\n\nObservation: Looking at the plot above, 2 distinct clusters stand out at first glance, cluster 4 and cluster 2:\n\nCluster 4: Include customers with high balance, cash_advance and credit_utilization. However, these customers have very low purchases. As we outlined earlier, these are customers that appear to be using their credit cards as a means to take out loans.\nCluster 2: Includes customers with high credit_utilization, slightly high balance and high purchases. These customers appear to be using their credits strictly for making purchases, and tend to pay off the balance frequently.\nCluster 3: These customers have lower values for all these features. However we know that this analysis cohort includes customers with tenure of 10 months, so these customers have had their credit cards for a while. This clusters appears to include customers who are cautious about using their credit cards and tend to make some low value purchases.\nCluster 1: These customers are similar to those in cluster 2, however they tend to have higher credit utilization. These customers appear to have much lower credit_limit (median of $2,500) compared to cluster 1 ($5,000) and cluster 3 ($3,500). Cluster 2 has the lowest median credit_limit ($1,350).\n\nLet us take a look at a few other feature distributions for our clusters. Below I show boxplots for credit_limit, minimum_payments, payments and purchase_trx for each cluster.\n\n\n\n\n\nObservation: Once again we can gleam a few insights:\n\nCluster 4: Appears to have higher credit_limit and also, customers in this cluster tend to make make higher minimum payments, suggesting that some of these customers (who are using their credit cards to take out loans) are indeed making efforts to pay off their balance.\nCluster 2: These customers have high the highest purchase_trx but also have payments,\nminimum_payment, credit_limit and similar to clusters 1 and 4. These customers while making frequent purchases with their credit tend to pay just the minimum required payments and thus always maintain some balance on their credit card.\nClusters 1 and 3: These customers appear to have similar characteristics and maintain low levels of use for their credit cards."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#customer-persona-product-recommendations",
    "href": "projects/customer_segmentation_banking/index.html#customer-persona-product-recommendations",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "5.4 Customer Persona / Product Recommendations",
    "text": "5.4 Customer Persona / Product Recommendations\nBased on the analysis in the previous section. We can thus create customer personas and come up with product recommendations to be marketed to these customers. We’ll do this using the following format; Cluster, Persona Name, Description, Stats, Product Recommendations.\n\nCluster 4: Loanee Customer\n\nDescription: These customers tend to use their credit cards a means for taking out loans. They use their credit card to bridge the gap between their financial needs, thus resulting in high cash advances, high balance and high credit utilization.\nStats: Range (medium - max).\n\nCredit Utilization - 65% - 97% (High)\nCash Advance - $3,431 - $11,221 (High)\nPurchases - $0 - $1,168 (Low)\nBalance - $2,701 - $9,560 (High)\n\nProduct Recommendations:\n\nBalance Transfer Card Options: Option to transfer their balance to 0% APR cards to save on interests rates.\nDebt Consolidation / Personal Loans: Provide debt consolidation loans with lower interest rates, allowing these customers to combine their high-interest debts into a single manageable payment, thus reducing their interest charges and simplifying their repayment strategy.\nCredit Counselling: Provide guidance on managing debt, creating budgets and financial improvements.\nCredit Builder Loans: Credit builder loans are specifically designed to help individuals build/improve their credit scores. Such loans are secured by the borrower’s savings account and timely payments are reported to the credit bureaus. These loans benefit both the financial institution through interest earned and also benfit the customer by establishing/rebuiliding positive credit history.\n\n\nCluster 2: Purchasing Customer\n\nDescription: These customers use their credit cards as a means for making frequent purchases. This may be due to the convenience and rewards earned by using their credit cards.\nStats: Range (medium - max).\n\nCredit Utilization - 35% - 93% (Medium to High)\nCash Advance - $49 - $4,161 (Low to High, excluding outliers)\nPurchases - $1,813 - $4,924 (High)\nBalance - $365 - $4,279 (Low to Medium)\n\nProduct Recommendations:\n\nRewards Programs: Reward programs where customers earn points, cashback, or airline miles based on their spending. These programs encourage customers to use their credit cards and provide incentive for loyalty.\nPremium Cards: This bank could offer premium cards with enhanced benefits for frequent users. While these cards typically come with higher annual fees (value for the bank), they also provide additional perks such as airport lounge access, concierge services, travel insurance, etc. (value for the customer).\nInstallment Plans: This bank could offer installment plans to these customers, enabling them to convert larger purchases in smaller, more manageable monthly payments.\nCredit Lines / Lower Interest Rates: Frequent card users who have established positive credit history could be eligible for increased credit lines and lower interest cards.\n\n\nCluster 3: Prudent Customer\n\nDescription: These customers take a cautious and thoughtful approach to their credit card usage. They have a sense of responsible financial behavior and consider their spending decisions carefully.\nStats: Range (medium - max).\n\nCredit Utilization - 4% - 52% (Low to Medium, excluding outliers)\nCash Advance - $0 - $1,465 (Low to Medium)\nPurchases - $460 - $3,431 (Low to Medium)\nBalance - $70 - $1,393 (Low to Medium)\n\nProduct Recommendations:\n\nLow Interest Cards: Offer credit cards with low annual percentage rates (APRs). These cards offer a lower cost of borrowing, allowing customers to manage their balances with minimal interest charges. Banks benefit from interest earned, while customers can maintain their cautious approach without worrying about excessive interest fees.\nSecured Credit Cards: Secured credit cards are an option for customers who want to build or rebuild their credit while maintaining control over their spending. These cards require a security deposit as collateral, which sets the credit limit. This offers an opportunity for cautious customers to establish a positive credit history while banks benefit from the security deposit and the potential to convert customers to unsecured credit cards in the future.\nFraud Protection and Monitoring: Banks can provide enhanced fraud protection and monitoring services for cautious customers. These services include real-time transaction alerts, identity theft protection, and advanced security features to safeguard against fraudulent activities. Customers gain peace of mind knowing that their accounts are closely monitored and protected, while banks can reduce the risk of financial losses due to fraud.\n\n\nCluster 1: Selective Customer\n\nDescription: These customers also have a careful and deliberate approach to credit card usage, primarily for low/medium value infrequent purchases. They are selective in their spending choices and prioritize making low/medium value purchases rather than frequent small transactions.\nStats: Range (medium - max).\n\nCredit Utilization - 13% - 98% (Low to High)\nCash Advance - $60 - $3,980 (Low to Medium)\nPurchases - $95 - $1,810 (Low to Medium)\nBalance - $365 - $4,279 (Low to Medium)\n\nProduct Recommendations:\n\nValue Purchase Financing: Banks can offer specialized financing options for infrequent purchases, allowing customers to pay for these purchases over time with low or 0% interest rates. This benefits customers by providing affordable installment plans and flexibility in managing their cash flow, while banks generate interest revenue and foster customer loyalty.\nExtended Warranty Protection: Banks can provide extended warranty protection as an added benefit for customers making certain infrequent high-value purchases. This extends the manufacturer’s warranty of these high-value purchases, offering coverage against unexpected repairs or replacements, providing peace of mind to customers. The bank benefits from increased card usage and customer satisfaction, potentially leading to long-term loyalty.\nPersonalized Spending Insights: Banks can offer personalized spending insights and analysis for customers who make infrequent purchases. This includes detailed transaction categorization, spending trends, and recommendations tailored to their unique spending patterns. It benefits customers by helping them make informed financial decisions and optimize their spending, while banks enhance customer engagement and strengthen relationships by providing valuable financial insights.\n\n\n\nThese are just a few examples of how this bank can build customer personas from the various customer segments/clusters and market financial products to them. It is important that these financial products are mutually beneficial to both the bank and the customers.\nThe bank can test the success of these financial products through various ways including experimentation (a/b testing), surveys, user analytics, net promoter score (NPS), pilot programs, etc."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#conclusion",
    "href": "projects/customer_segmentation_banking/index.html#conclusion",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, customer segmentation and persona creation using the k-means algorithm have emerged as powerful tools in the banking industry. By leveraging data analytics and machine learning techniques, banks can gain a deeper understanding of their customers, tailor their offerings to specific segments, and deliver personalized experiences. The k-means algorithm enables the identification of distinct customer groups based on behavioral patterns and characteristics, allowing banks to develop targeted strategies, design relevant financial products, and enhance customer satisfaction. With customer segmentation and persona creation at the core of their business strategies, banks can foster stronger customer relationships, drive growth, and stay ahead in today’s competitive landscape."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#data-prep",
    "href": "projects/customer_segmentation_banking/index.html#data-prep",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Data Prep",
    "text": "Data Prep\nTo prepare the dataset for k-means clustering, I performed the following steps:\n\nCreate an analysis cohort by filtering for customers with tenure = 10.\nDrop any rows with null values. This leaves an analysis cohort of 226 customers.\nCreate a credit_utilization feature by dividing balance by credit_limit."
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#feature-engineering",
    "href": "projects/customer_segmentation_banking/index.html#feature-engineering",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nFeature engineering is an important step in preparing data for k-means clustering. I used the following techniques for feature engineering in order to improve the accuracy and effectiveness of the algorithm:\n\nRemove unwanted features - Remove the cust_id column. This is not needed for k-means and is used only as a customer identifier. I also make the executive decision to remove the installment_purchases column. This feature is defined as the “amount of purchases done in installments”. I would assume that purchases is the important feature in this dataset and regardless of the number of purchase installments.\nNormalization - This step scales each feature to have a mean of zero and standard deviation of one. This helps prevent features with larger values from dominating the clustering results.\nDimensionality Reduction (PCA) - Reducing the dimensionality of the data can reduce the computational complexity and improve the interpretability of the clustering results. A popular dimensionality reduction technique is called PCA (Principal Component Analysis). PCA works by identifying a smaller number of variables, known as principal components, that can explain the maximum amount of variance in the original dataset. These principal components are linear combinations of the original variables, and each one represents a different direction in the data. The first principal component captures the most variance in the data, while each subsequent component captures as much of the remaining variance as possible. For a video explanation of how PCA works, see this link.\n\nSetting a PCA threshold of 80%, meaning generating enough components to capture 80% of the variability in the features, PCA identified 5 principal components from 16 features. See sample below;"
  },
  {
    "objectID": "projects/customer_segmentation_banking/index.html#optimal-number-of-clusters",
    "href": "projects/customer_segmentation_banking/index.html#optimal-number-of-clusters",
    "title": "Customer Segmentation & Persona Creation in the Banking Industry",
    "section": "Optimal Number of Clusters",
    "text": "Optimal Number of Clusters\nTo determine the optimal number of clusters, we can use a Skree Plot. A skree plot is used to determine the appropriate number of clusters to use in a k-means clustering analysis. The skree plot displays the within-cluster sum of squares (WCSS) on the y-axis, plotted against the number of clusters on the x-axis.\nThe within-cluster sum of squares measures the total squared distance between each point and it’s assigned cluster center, summed over all clusters. The scree plot helps identify the elbow point, or the point on the plot where adding more clusters does not significantly improve the clustering performance, i.e., the WCSS is not significantly reduced by adding more clusters.\nThe elbow point is typically used as a guide for deciding how many clusters to retain in the analysis. However, there is no hard and fast rule for selecting the number of clusters, this can be based on domain knowledge and knowledge about the dataset. The plot below is the skree plot for our analysis cohort;\n\n\n\n\n\nObservation: We can see that the elbow point appears to be 5 clusters. However after running the first iteration of the k-means algorithm, I noticed that only 1 customer (cust_id C11004) was being assigned to cluster 1. I would assume that this customer has certain characteristics that stand out for the rest of the cohort. For this particular analysis, I just decided to exclude this customer and re-cluster again, using 4 clusters."
  },
  {
    "objectID": "index.html#hi-im-lucas",
    "href": "index.html#hi-im-lucas",
    "title": "Lucas Okwudishu",
    "section": "",
    "text": "I am a data and analytics professional with over 10 years of experience in business operations and analytics. I have expertise in multiple industries including retail, manufacturing, non-profit and marketing.\nI have a proven track record of improving business operations and driving ROI true insights and action.\nI love digging into data to understand how various business components affect top-line revenue. This helps with better revenue modeling as well as understanding what levers can be used to drive revenue while eliminating inefficiencies."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Explore My Data Science Portfolio",
    "section": "",
    "text": "Discover how I harness the power of data and analytics through machine learning, predictive analytics, and insightful visualizations. Each project is a unique story of challenges met with innovative solutions, crafted to turn complex data into actionable insights.\n\n\n\n\n\n\nNote\n\n\n\nShorter excerpt of some of these projects are available on Medium\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAdvanced Predictive Analytics for Managing Overnight Shelter Capacity\n\n\nUnifying Big Query DBT, H2O, and APIs for Social Good\n\n\n\n\npredictive analytics\n\n\nmachine learning\n\n\ndata modeling\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nLucas Okwudishu\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Hourly Bike Rentals\n\n\nIn Persuit of Model Accuracy\n\n\n\n\npredictive analytics\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nLucas Okwudishu\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Segmentation & Persona Creation in the Banking Industry\n\n\nA Case Study Using K-Means Algorithm\n\n\n\n\ncustomer segmentation\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nLucas Okwudishu\n\n\n\n\n\n\n  \n\n\n\n\nStrategic Bike Pricing: Harnessing Machine Learning for Market Edge\n\n\nAn Analytic Approach to Competitive Price Modeling in Bike Manufacturing\n\n\n\n\nmachine learning\n\n\nweb scraping\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\nLucas Okwudishu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/toronto_shelter_project/index.html",
    "href": "projects/toronto_shelter_project/index.html",
    "title": "Advanced Predictive Analytics for Managing Overnight Shelter Capacity",
    "section": "",
    "text": "Introduction\nIn the dynamic urban landscape of Toronto, overnight shelters represent more than just a temporary haven for the homeless. These facilities are a vital component of the city’s social support network, providing safety and stability for individuals and families facing the harsh realities of urban life. As an essential service, these shelters embody the collective effort to uphold the welfare of all citizens, ensuring that the city’s commitment to caring for its most vulnerable members is not just a promise, but a reality.\n\n\nProblem Statement\nThe challenge lies in effectively forecasting the occupancy rates of these shelters. Accurate predictions are not merely statistical triumphs; they are the an important tool in the complex machinery of social support, determining the operational efficacy and resource optimization of these critical infrastructures. The implications of such forecasts extend beyond the confines of the shelters themselves, influencing policy-making, urban planning, and emergency response initiatives. The margin for error is narrow: underestimate, and we risk leaving individuals or families exposed to the elements. Overestimate, and we divert precious resources that could be utilized more effectively elsewhere. The quest for a reliable prediction model is not just a technical endeavor, it is a moral imperative to ensure that the well being of those in need is safeguarded.\n\n\nSolution Strategy\nThe proposed solution leverages the power of machine learning to analyze historical shelter occupancy data and generate accurate forecasts for overnight shelter occupancy. The approach is to train sophisticated ML models that can identify patterns and trends from past data, considering various factors such as location, day of the week, month of the year, weather conditions, etc. By feeding the models with historical data, they learn to anticipate future demands with a considerable degree of accuracy. This predictive capability is enhanced by incorporating advanced algorithms and ensemble methods that can adapt to the changing dynamics of urban life. The end goal is a robust model capable of aiding shelter administrators to anticipate nightly occupancy rates and thus manage their operations with unprecedented foresight and efficiency.\n\n\nImplementation Strategy\n\n\n\n\n\n\nData Collection and Management:\n\nUtilize the Toronto Open Data API to collect historical occupancy data from multiple overnight shelters.\nGather additional relevant datasets such as historical weather patterns from AccuWeather API.\n\nData Storage:\n\nStore the collected data securely in Google BigQuery, ensuring the database scales with the inflow of new data and is optimized for complex analytical queries.\n\nData Transformation:\n\nImplement DBT (Data Build Tool) to perform and manage necessary data transformations, ensuring that the data is clean, structured, and ready for model consumption.\n\nMachine Learning Model Development:\n\nUtilize H2o AutoML, an automated machine learning platform, to build and train predictive models.\nEvaluate various models and their performances, tuning hyper-parameters and selecting the best-performing model for deployment.\n\nPrediction Deployment:\n\nDeploy the trained machine learning model via a Shiny app.\nImplement robust app features that allow for real-time updates and the monitoring of model performance over time.\n\n\n\n\n\n\n\n\nShiny App\n\n\n\nYour can interact with the shiny app here.\n\n\nBy following this implementation strategy, this project aims to harness the vast potential of ML to deliver a solution that not only serves the immediate needs of overnight shelters in Toronto but also sets a standard for data-driven decision-making in humanitarian efforts city-wide.\nThe following sections walk through each step of the implementation strategy in more detail.\n\n\n\nData Collection\nThe first phase of the project involves gathering historical raw data on overnight shelter occupancy using Open Data APIs, which is then uploaded to BigQuery using an R script. This includes a one-time data upload for historical data from 2022 and ongoing daily updates for 2023. To enhance our prediction model, we also collect five-day weather forecasts for the Toronto area from the AccuWeather API. This weather information is a key factor in predicting shelter needs over the next five days, along with the occupancy data.\n\n\n\nData Storage\nAs mentioned earlier, this raw data in stored in a BigQuery project, in a dataset called “raw_data”. This dataset is also used to store historical daily weather data which is used as part of features for predicting overnight shelter occupancy.\n\n\n\nData Transformation\nThe data transformation phase is critical to ensuring data quality and readiness for subsequent analysis and predictive modeling. Leveraging DBT, a set of data models are created to convert raw datasets into a refined form and stored in a dataset named clean_data.\n\n\n\nFigure 1: Sample DBT model lineage\n\n\nBelow is an explanation of what each DBT model does;\n\nData Cleaning with DBT Models:\n\nshelter_occupancy_2022_2023: This model consolidates raw shelter occupancy records from 2022 and 2023. It performs a pivot transformation, changing the dataset from a wide format (with many columns) to a long format (with rows representing each data point), facilitating easier manipulation and analysis.\nshelter_occupancy_id_mapping: To standardize and uniquely identify characteristics within the data, this model assigns IDs to distinguishable attributes like program model and sector. This process aids in maintaining data integrity and consistency.\nshelter_occupancy_2022_2023_flagged_id: Building on the prior model, this model crafts unique primary keys that map each data row to a specific shelter location and program offering, thereby enabling precise tracking and referencing within the dataset.\nweather_historical_2022_2023: Here, historical weather information for 2022 and 2023 is merged.\n\nPreparing Data for Machine Learning:\n\nshelter_occupancy_weather_integration_2022_2023: This model integrates daily shelter occupancy figures with corresponding weather metrics for each particular date. It ensures completeness by employing a 3-day rolling average to impute any gaps in the weather dataset, thereby maintaining the continuity and reliability of the data for machine learning purposes.\n\nFeature Engineering for Predictive Modeling:\n\nshelter_occupancy_forecast_features: Targeting predictive analytics, this model generates forward-looking features essential for forecasting shelter occupancy. It projects the next five days’ dates for each shelter and program pairing and merges in the weather forecast for the same horizon. The result is a feature-rich dataset, crafted to anticipate future occupancy rates.\n\n\n\n\n\nMachine Learning & Model Development\nIn this phase, data is imported from BigQuery into R where machine learning models are built to predict overnight shelter occupancy. A few caveats in this phase -\n\nIn this beta phase, I focused on creating predictions for the top 3 organizations with the highest number of locations. These include City of Toronto (Org ID 1), Homes First Society (Org ID 15) and Dixon Hall (Org ID 6). All together, these 3 organizations have about 40 distinct locations. Again in a later phase, I hope to expand this project to include all locations.\nI also focused on only modeling locations that have been open (in 2022 and 2023) for at least 75% of the year.\nAccording to the information in the shelter occupancy dataset, there are two measures of capacity; Funding capacity reports the number of beds or rooms that a program is intended to provide. Actual capacity reports the number of beds or rooms in service and showing as available for occupancy in the shelter management information system at the time of reporting. There are a number of reasons why beds or rooms may be temporarily out of service, including maintenance, repairs, renovations, etc. Thus for making future predictions, a 7 day average of Actual capacity was used to estimate actual capacity. Also if the model’s prediction of overnight shelter occupancy for beds or rooms (occupied) exceeded the actual capacity, the occupied value was adjusted down to the actual capacity value. For example if the actual capacity for a beds in a particular location was estimated to be 70, and the model predicted occupied beds for this location on a given night to be 73, this predicted value of 73 was adjusted down to 70 (to match the actual capacity), and thus resulting in predicted occupancy rate of 100%.\n\nAs mentioned earlier, H2o AutoML was used to create machine learning models. The model currently being used for making predictions is the H2o AutoML leader (StackedEnsemble_BestOfFamily_1_AutoML_2_20231026_61754). A Stacked Ensemble is a model that combines the strengths of multiple algorithms to make predictions. Looking at the metrics, the training data reflects an extremely promising model with very low error rates; MSE of 0.0648, RMSE of 0.2554, MAE of 0.1737.\n\n\n\nFigure 2: H2o AutoML Leaderboard\n\n\nHowever, a markedly different picture is painted when we consider the validation data metrics, which reveal an MSE of 152.8 and an RMSE of 12.3630. The MAE on the validation set is 7.1667. The large discrepancy between the training and validation metrics raises concerns about the model’s ability to generalize, suggesting that it may have over-fitted to the training data.\nCross-validation metrics serve as a middle ground, offering a more generalized assessment of model performance. These are less optimistic than the training metrics but far better than the validation metrics. With a cross-validated RMSE of 0.9291 and MAE of 0.3556, the model demonstrates decent predictive ability on data it was not trained on. However, the higher standard deviation in these cross-validated metrics suggests some variability in the model’s performance across different subsets of the data.\nIt is essential to note that discrepancies between training and validation metrics, such as those observed, could potentially signal data leakage, overfitting, or issues with the data splits, all of which are aspects of the pipeline that require careful attention. However, such concerns will be addressed in the subsequent iterations of the project, where model accuracy and generalization become the central focus.\n\n\n\n\n\n\nNote\n\n\n\nThe immediate goal is to establish that the pipeline correctly processes data through all its stages, ensuring that the model receives the input in the expected format and that the output is appropriately captured and stored. This verification is crucial for scalability and reliability before fine-tuning the model’s predictive capabilities. Once this is confirmed, iterative improvements on the model, such as hyperparameter tuning, cross-validation strategy adjustments, and feature selection, will be undertaken to optimize the model for accuracy and robustness.\n\n\n\n\n\nPredictions\nBefore predictions can be made, an essential data preparation step is conducted using a DBT model named shelter_occupancy_forecast_features. This DBT model is responsible for transforming raw data into a structured format that can be utilized for making predictions, essentially generating features that will feed into the stacked ensemble model. The step-by-step process is described below;\n\nFeature Engineering in BigQuery: The raw data is processed by the DBT model within BigQuery, The shelter_occupancy_forecast_features model applies necessary transformations and engineering to prepare the dataset with features suitable for the predictive model.\nData Import into R: Once the features are ready, the processed dataset is imported into an R environment.\nPrediction with the Stacked Ensemble Model: Within R, the stacked ensemble model is applied to the feature set to generate forecasts. Given the model’s composition from the AutoML run, it utilizes its combination of base learners and meta-learner to output highly informed predictions.\nExport Predictions to BigQuery: The predictions made by the ensemble model are then exported back into BigQuery. This reintegration allows for seamless storage and potential further analysis within the cloud data warehouse environment.\n\nThis pipeline exemplifies a modern analytical workflow, leveraging the strengths of various technologies; BigQuery for scalable data storage and preprocessing, DBT for data transformation and feature engineering, R for statistical modeling and predictions, and H2O for advanced machine learning techniques. By loading the predictive results back into BigQuery, the data becomes readily accessible for business intelligence tools, further analytics, or downstream applications, ensuring a cohesive and continuous flow of data from raw inputs to actionable outputs.\n\n\n\nDeployment\n\n\n\n\n\nThe deployment of the shelter occupancy forecast into a Shiny application represents the culmination of the data pipeline and the predictive modeling process. Shiny is a highly flexible R package that enables the creation of interactive web applications directly from R. Here’s how the final stage unfolds:\n\nImporting Predictions: The occupancy predictions, which have been computed by the H2o Stacked Ensemble model and stored in BigQuery, are imported into the Shiny application environment.\nInteractive User Interface: The Shiny application is developed with a user-friendly interface that presents the forecast data to end-users. It will feature:\n\nData Table: An interactive table displaying the forecasted overnight shelter occupancy across multiple locations and programs. This table allows for sorting, searching, and perhaps filtering to enable users to easily navigate through the data.\nMap Visualization: A map component will visualize the different shelter locations. The map will be color-coded based on predicted occupancy rates, providing an intuitive, at-a-glance understanding of the data. Locations can be marked with pins or regions shaded to reflect the predicted occupancy, giving users a geographical context.\n\nUser Interaction and Customization: Users will have the ability to select different dates, locations, or other relevant parameters to refine the predictions displayed. The Shiny app’s reactivity will ensure that any changes in user input will automatically update the visualizations and data presented.\n\nThis Shiny application will serve as a decision support tool, enabling stakeholders and decision-makers to access predictive insights in a user-friendly format. It provides a practical means for users to explore and utilize the model’s forecasts to manage shelter occupancy more effectively, ultimately aiding in resource planning and allocation for the locations and programs in question.\n\n\n\nConclusion\nThis project highlights the potential of predictive analytics in the realm of social good. By predicting shelter occupancy rates with greater accuracy, this kind of analysis not only helps cities optimize resource allocation but also serves as a beacon of hope, ensuring that shelters can prepare for and meet the needs of the most vulnerable members of society. It’s a vivid illustration of how data, technology, and human compassion can intersect to create a profound and positive social impact.\nAdditionally, this project demonstrates the practical implementation of a real-world analytics pipeline, from data collection to the complexities of model deployment. By leveraging public APIs, coupled with the analytical capabilities of R, the robust storage of BigQuery, and the power of DBT for data modeling, we’ve demonstrated a successful proof of concept.\nFinally this project also demonstrates some key competencies including business understanding that guides the project’s direction, data gathering and working with APIs, sophisticated data engineering to sculpt raw information into actionable insights, advanced machine learning to forecast outcomes accurately, and effective model deployment to bring these insights into operational use.\n\n\n\nAdditional Next Steps\nThere are still a few things that could enhance this project as well as some improvements that could be made to the current workflow;\n\nAccuracy Tracking:\n\nScript for Tracking Predictions vs. Actuals: Develop and integrate a script within the R environment that routinely compares the predicted values of overnight shelter occupancy with the actual figures as they become available. include statistical measures as MAE, RMSE to compute accuracy of predictions in production.\nAlerts for Deviations: Implement an alert system that notifies the team when the discrepancy between predicted and actual values exceeds a predefined threshold, indicating a potential drift in model performance or significant changes in the underlying data.\n\nScale Model Training and Predictions to all Locations:\n\nCurrently, a few selected locations are being used in this project. The end goal is to scale modeling and predictions to all locations.\n\nBetter Model Experiment Tracking:\n\nIncorporate model tracking and experiment logging capabilities using a tool like MLflow.\n\nEnhance Shiny Application Enhancement with Modules:\n\nModular Design: Refactor the existing Shiny app into a modular design. Shiny modules are reusable parts of a Shiny app that can simplify app development and maintenance. They help in organizing the code, especially for complex applications with multiple reactive outputs and inputs.\n\n\n\n\nReproducible Code\n\nR Project.\nDBT Project."
  },
  {
    "objectID": "projects/bike_rental_prediction/index.html",
    "href": "projects/bike_rental_prediction/index.html",
    "title": "Predicting Hourly Bike Rentals",
    "section": "",
    "text": "A data science project to predict hourly bike rentals in the city of Seoul using date and weather seasons. Read analysis write up on Medium"
  },
  {
    "objectID": "projects/bike_price_recommender/index.html",
    "href": "projects/bike_price_recommender/index.html",
    "title": "Strategic Bike Pricing: Harnessing Machine Learning for Market Edge",
    "section": "",
    "text": "In the competitive bike manufacturing industry, pricing strategies can significantly impact market share and profitability. A hypothetical bike manufacturer faces the challenge of setting prices that are competitive yet profitable, adapting to market demands without compromising margins. To stay competitive, the manufacturer needs a data-driven approach to dynamically adjust prices based on bike features and market trends."
  },
  {
    "objectID": "projects/bike_price_recommender/index.html#implementation",
    "href": "projects/bike_price_recommender/index.html#implementation",
    "title": "Strategic Bike Pricing: Harnessing Machine Learning for Market Edge",
    "section": "Implementation",
    "text": "Implementation\n\nData Collection: Data was gathered through web scraping from the Trek bikes website, ensuring a rich dataset for analysis.\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\nWeb-scraping script available here.\n\n\n\nFeature Engineering: Bike attributes such as model, year, frame material, and additional features like electrical components and shock presence were encoded. Keywords indicative of special features and parts manufacturers were also extracted to enrich the dataset.\n\n\n\n\n\n\n\nData Prep\n\n\n\nData prep script available here.\n\n\n\nModel Development: Four machine learning models were chosen for price prediction: RANDOM FOREST, XGBOOST, GLMNET (Linear Regression), and MARS (Multivariate Adaptive Regression Splines). Model Training and Validation: Each model was trained using the prepared dataset, validated, and its pricing predictions were compared to actual prices to gauge competitiveness.\nStrategy Formulation: The RANDOM FOREST model was designated as the standard “every day” pricing model due to its accurate price predictions. The XGBOOST model was chosen for price markups given its higher price points, while the GLMNET and MARS models were selected for markdown scenarios due to their lower price predictions. Deployment:\n\n\n\n\n\n\n\nModeling\n\n\n\nModeling script available here.\n\n\n\nDeployment: A user-friendly shiny app was developed to deploy the analysis. The app allows users to select bike specifications and select a pricing model to receive price recommendations.\n\n\n\n\n\n\n\nShiny App\n\n\n\nCode for shiny app available here."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Coming Soon…\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]